{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8437e793-a0ef-4391-af16-c70fa550ac4b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import typing as ty\n",
    "import random\n",
    "import math\n",
    "import json\n",
    "import gzip\n",
    "from pathlib import Path\n",
    "import itertools as it\n",
    "import operator as op\n",
    "\n",
    "import more_itertools as mit\n",
    "import plotly.express as px\n",
    "import numpy as np\n",
    "import numpy.typing as npt\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2318bcc1",
   "metadata": {},
   "source": [
    "Set device to be run on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea01968f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "# torch.set_num_threads(4) ## For when Jacan's running it\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print (device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "351f728e-89c8-4ded-812d-41887b5a9717",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def data_iter(path: Path) -> ty.Iterator[ty.Dict[str, ty.Any]]:\n",
    "    with gzip.open(path, \"rt\") as f:\n",
    "        data_list = json.load(f)\n",
    "        random.shuffle(data_list)\n",
    "        for data_dict in data_list:\n",
    "            yield data_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d418a5-3148-47f5-a446-4e60ff5d7fa5",
   "metadata": {},
   "source": [
    "# Data pipeline\n",
    "\n",
    "Here we define `bin_power()`, which effectively just splits the frequency axis into equal sized, contiguous bins, and then adds the powers of all the measurements which fall into that bin. This routine provides a way of changing the resolution of the signal to match the size of the input layer.\n",
    "\n",
    "We also define `example_loader()`, which is a _generator function_, that can be iterated to create labelled training examples to be passed to the model and loss functions.\n",
    "\n",
    "Note: the synthetic data generated in this notebook is actually sine waves in real space, not frequency space, but it should work the same either way. This choice was just made for quick prototyping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d3f27920-240b-40b4-a6d0-5e974117b13a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def bin_power(\n",
    "    freqs: npt.NDArray[np.float64],\n",
    "    powers: npt.NDArray[np.float64],\n",
    "    num_bins: int,\n",
    "    freq_range: ty.Tuple[float, float],\n",
    ") -> npt.NDArray[np.float64]:\n",
    "    \"\"\"Bins power spectrum with frequency.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    freqs : ndarray[float64]\n",
    "        Frequency values (x-axis).\n",
    "    powers : ndarray[float64]\n",
    "        Power values for given frequency (y-axis).\n",
    "    num_bins : int\n",
    "        Number of values to downsample the power spectrum to.\n",
    "    freq_range : tuple[float, float]\n",
    "        Range of values for the frequency (x-axis).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    ndarray[float64]\n",
    "        Bin-summed frequency spectrum.\n",
    "    \"\"\"\n",
    "    freq_bins = np.linspace(*freq_range, num_bins + 1)\n",
    "    signal = np.zeros(num_bins, dtype=np.float64)\n",
    "    idxs = np.digitize(freqs, freq_bins, right=True) - 1\n",
    "    np.add.at(signal, idxs, powers)\n",
    "    return signal, freq_bins\n",
    "\n",
    "\n",
    "def example_loader(\n",
    "    dict_iter: ty.Iterator[ty.Dict[str, ty.Any]], resolution: int,\n",
    ") -> ty.Iterator[ty.Tuple[torch.Tensor, torch.Tensor, npt.NDArray[np.float64]]]:\n",
    "    \"\"\"Provides samples from our training set of input data,\n",
    "    and the target.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    This is where the data is read from file, and provided\n",
    "    for use in the training loop.\n",
    "    You should should extend this function to shuffle the\n",
    "    data training example ordering.\n",
    "    \"\"\"\n",
    "    freq_range = (0.0, 25.0)\n",
    "    for data_dict in dict_iter:\n",
    "        freqs = np.array(data_dict.pop(\"frequency\"), dtype=np.float64)\n",
    "        power = np.array(data_dict.pop(\"power\"), dtype=np.float64)\n",
    "        power = np.nan_to_num(power, copy=False)\n",
    "        # power /= power.sum()\n",
    "        target_val = int(data_dict.pop(\"target\"))\n",
    "        signal, bins = bin_power(freqs, power, resolution, freq_range)\n",
    "        signal = torch.from_numpy(signal)\n",
    "        target = torch.zeros(2, dtype=torch.float64)\n",
    "        target[target_val] = 1\n",
    "        # For running on GPU\n",
    "        signal = signal.cuda()\n",
    "        target = target.cuda()\n",
    "        yield signal, target, bins\n",
    "\n",
    "\n",
    "# LoaderElement = ty.Tuple[torch.Tensor, torch.Tensor, np.ndarray]\n",
    "\n",
    "\n",
    "# def batch_examples(loader: ty.Iterable[LoaderElement], batch_size: int) -> ty.Iterator[LoaderElement]:\n",
    "#     for chunk in mit.chunked(loader, batch_size):\n",
    "#         signals, targets, bin_batch = zip(*chunk)\n",
    "#         signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff4fb756-0094-41d6-890c-cec3d0a589fa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'signals' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_11888/74810637.py\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msignals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m25\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'signals' is not defined"
     ]
    }
   ],
   "source": [
    "tuple(map(lambda x: torch.any(torch.isnan(x)).item(), signals)).index(True, 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "551f8419-923b-4f1c-bc98-4bd5990b1b5a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "json_data = data_iter(\"./DATA/jsons/training_data.json.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bf25a9a3-ee5c-4068-be98-3280abb627cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "foo = tuple(it.islice(json_data, 40))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "60a212e1-669e-4ab2-9948-5281b7cb900b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_dict = foo[31]\n",
    "freqs = np.array(data_dict.pop(\"frequency\"), dtype=np.float64)\n",
    "power = np.array(data_dict.pop(\"power\"), dtype=np.float64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28fd481c-db98-4376-b5a2-466a2d3777f5",
   "metadata": {},
   "source": [
    "# Model and optimization definitions\n",
    "\n",
    "Here we provide constructor functions, which create the model (a very simple 2 layer MLP), along with the loss and optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1635e326-6c3e-4256-9b5f-7c85df046d0d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ModelType = torch.nn.Module\n",
    "OptimType = torch.optim.Optimizer\n",
    "LossType = torch.nn.modules.loss._WeightedLoss\n",
    "\n",
    "\n",
    "def model_init(\n",
    "    in_dim: int, hidden_dim: int, out_dim: int, depth: int,\n",
    ") -> ModelType:\n",
    "    model = torch.nn.Sequential(\n",
    "        torch.nn.Linear(in_dim, hidden_dim),\n",
    "        torch.nn.Dropout(p=0.2),\n",
    "    )\n",
    "    for i in range(depth):\n",
    "        model.append(torch.nn.PReLU())\n",
    "        model.append(torch.nn.Linear(hidden_dim, hidden_dim if (i + 1) < depth else out_dim))\n",
    "    model.append(torch.nn.Softmax(-1))\n",
    "    model.to(device) # Added for running on GPU\n",
    "    return model.to(torch.float64)\n",
    "\n",
    "\n",
    "def optim_init(\n",
    "    model: ModelType, lr: float, weight_decay: float\n",
    ") -> ty.Tuple[OptimType, LossType]:\n",
    "    return (\n",
    "        torch.optim.SGD(\n",
    "            model.parameters(),\n",
    "            lr=lr,\n",
    "            momentum=0.3,\n",
    "            weight_decay=weight_decay,\n",
    "        ),\n",
    "        torch.nn.CrossEntropyLoss(),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e44653ae-2e79-4e21-a934-bcc66fc27ad3",
   "metadata": {},
   "source": [
    "# Training loop\n",
    "\n",
    "This routine runs a single epoch of a training loop. It is to be passed the results of the model and optimizer constructor functions, along with the dataloader generator. Additionally, `interval` controls how many examples to calculate the accuracy over, and `converge_limit` sets the number of consecutive perfect accuracy scores to require before exiting the training loop. If this is not met, the model will keep training until the dataloader is exhausted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b0a0ff9e-cfe7-41c6-844f-0bcbda87469e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train(\n",
    "    model: ModelType,\n",
    "    optim: OptimType,\n",
    "    criterion: LossType,\n",
    "    dataloader: ty.Iterator[ty.Tuple[torch.Tensor, torch.Tensor, npt.NDArray[np.float64]]],\n",
    "    interval: int = 100,\n",
    "    converge_limit: ty.Optional[int] = 10,\n",
    ") -> ModelType:\n",
    "    num_correct = 0\n",
    "    converge_count = 0\n",
    "    model.train()\n",
    "    for i, (signal, target, _) in enumerate(dataloader):\n",
    "        optim.zero_grad()\n",
    "        pred = model(signal)\n",
    "        if torch.argmax(pred) == torch.argmax(target):\n",
    "            num_correct += 1\n",
    "        loss = criterion(pred, target)\n",
    "        if (i % interval == 0) and (i != 0):\n",
    "            acc = num_correct / interval\n",
    "            print(f\"training step: {i:07d}, loss: {loss.item():.5f}, accuracy: {acc:.5f}\")\n",
    "            num_correct = 0\n",
    "            if math.isclose(acc, 1.0):\n",
    "                converge_count += 1\n",
    "            else:\n",
    "                converge_count = 0\n",
    "            if (converge_limit is not None) and (converge_count == converge_limit):\n",
    "                break\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa3a911-2b93-4f22-8c35-f68679a7f2f5",
   "metadata": {},
   "source": [
    "# Defining and training the model\n",
    "\n",
    "We pass in the model and optimizer hyperparameters, initialise the dataloader, and then run the training loop. The model is returned from the training loop for later evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8db81502-45aa-4e2c-b9ae-36c2552451ed",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=0\n",
      "training step: 0000100, loss: 0.68631, accuracy: 0.97000\n",
      "training step: 0000200, loss: 0.68404, accuracy: 0.91000\n",
      "training step: 0000300, loss: 0.68201, accuracy: 0.87000\n",
      "training step: 0000400, loss: 0.70660, accuracy: 0.88000\n",
      "training step: 0000500, loss: 0.67765, accuracy: 0.92000\n",
      "training step: 0000600, loss: 0.67577, accuracy: 0.85000\n",
      "training step: 0000700, loss: 0.71307, accuracy: 0.89000\n",
      "training step: 0000800, loss: 0.67148, accuracy: 0.91000\n",
      "training step: 0000900, loss: 0.66941, accuracy: 0.89000\n",
      "training step: 0001000, loss: 0.66741, accuracy: 0.88000\n",
      "training step: 0001100, loss: 0.66542, accuracy: 0.88000\n",
      "training step: 0001200, loss: 0.66333, accuracy: 0.90000\n",
      "training step: 0001300, loss: 0.66152, accuracy: 0.85000\n",
      "training step: 0001400, loss: 0.65945, accuracy: 0.90000\n",
      "training step: 0001500, loss: 0.73018, accuracy: 0.88000\n",
      "training step: 0001600, loss: 0.65570, accuracy: 0.85000\n",
      "training step: 0001700, loss: 0.65373, accuracy: 0.89000\n",
      "training step: 0001800, loss: 0.65154, accuracy: 0.93000\n",
      "training step: 0001900, loss: 0.64937, accuracy: 0.93000\n",
      "training step: 0002000, loss: 0.64716, accuracy: 0.94000\n",
      "training step: 0002100, loss: 0.64523, accuracy: 0.89000\n",
      "training step: 0002200, loss: 0.64318, accuracy: 0.92000\n",
      "training step: 0002300, loss: 0.64125, accuracy: 0.90000\n",
      "training step: 0002400, loss: 0.63933, accuracy: 0.90000\n",
      "training step: 0002500, loss: 0.75172, accuracy: 0.81000\n",
      "training step: 0002600, loss: 0.63584, accuracy: 0.93000\n",
      "training step: 0002700, loss: 0.63426, accuracy: 0.84000\n",
      "training step: 0002800, loss: 0.75783, accuracy: 0.89000\n",
      "training step: 0002900, loss: 0.63080, accuracy: 0.86000\n",
      "training step: 0003000, loss: 0.62926, accuracy: 0.84000\n",
      "training step: 0003100, loss: 0.62762, accuracy: 0.86000\n",
      "training step: 0003200, loss: 0.62624, accuracy: 0.81000\n",
      "training step: 0003300, loss: 0.62474, accuracy: 0.84000\n",
      "epoch=1\n",
      "training step: 0000100, loss: 0.62217, accuracy: 0.92000\n",
      "training step: 0000200, loss: 0.62044, accuracy: 0.89000\n",
      "training step: 0000300, loss: 0.61836, accuracy: 0.96000\n",
      "training step: 0000400, loss: 0.61669, accuracy: 0.88000\n",
      "training step: 0000500, loss: 0.61509, accuracy: 0.87000\n",
      "training step: 0000600, loss: 0.61335, accuracy: 0.90000\n",
      "training step: 0000700, loss: 0.61156, accuracy: 0.91000\n",
      "training step: 0000800, loss: 0.61004, accuracy: 0.86000\n",
      "training step: 0000900, loss: 0.60845, accuracy: 0.88000\n",
      "training step: 0001000, loss: 0.60670, accuracy: 0.91000\n",
      "training step: 0001100, loss: 0.78945, accuracy: 0.83000\n",
      "training step: 0001200, loss: 0.60376, accuracy: 0.89000\n",
      "training step: 0001300, loss: 0.60216, accuracy: 0.89000\n",
      "training step: 0001400, loss: 0.60043, accuracy: 0.92000\n",
      "training step: 0001500, loss: 0.79700, accuracy: 0.83000\n",
      "training step: 0001600, loss: 0.59757, accuracy: 0.89000\n",
      "training step: 0001700, loss: 0.59598, accuracy: 0.90000\n",
      "training step: 0001800, loss: 0.59457, accuracy: 0.86000\n",
      "training step: 0001900, loss: 0.59332, accuracy: 0.83000\n",
      "training step: 0002000, loss: 0.59172, accuracy: 0.91000\n",
      "training step: 0002100, loss: 0.59021, accuracy: 0.89000\n",
      "training step: 0002200, loss: 0.58876, accuracy: 0.88000\n",
      "training step: 0002300, loss: 0.58741, accuracy: 0.86000\n",
      "training step: 0002400, loss: 0.81314, accuracy: 0.86000\n",
      "training step: 0002500, loss: 0.58460, accuracy: 0.89000\n",
      "training step: 0002600, loss: 0.81656, accuracy: 0.84000\n",
      "training step: 0002700, loss: 0.58200, accuracy: 0.87000\n",
      "training step: 0002800, loss: 0.58043, accuracy: 0.92000\n",
      "training step: 0002900, loss: 0.57921, accuracy: 0.84000\n",
      "training step: 0003000, loss: 0.57770, accuracy: 0.91000\n",
      "training step: 0003100, loss: 0.57610, accuracy: 0.93000\n",
      "training step: 0003200, loss: 0.57461, accuracy: 0.91000\n",
      "training step: 0003300, loss: 0.82916, accuracy: 0.83000\n",
      "epoch=2\n",
      "training step: 0000100, loss: 0.57120, accuracy: 0.93000\n",
      "training step: 0000200, loss: 0.56996, accuracy: 0.86000\n",
      "training step: 0000300, loss: 0.56859, accuracy: 0.89000\n",
      "training step: 0000400, loss: 0.56728, accuracy: 0.88000\n",
      "training step: 0000500, loss: 0.56576, accuracy: 0.93000\n",
      "training step: 0000600, loss: 0.84071, accuracy: 0.84000\n",
      "training step: 0000700, loss: 0.56305, accuracy: 0.95000\n",
      "training step: 0000800, loss: 0.56165, accuracy: 0.91000\n",
      "training step: 0000900, loss: 0.56046, accuracy: 0.86000\n",
      "training step: 0001000, loss: 0.55933, accuracy: 0.85000\n",
      "training step: 0001100, loss: 0.55806, accuracy: 0.88000\n",
      "training step: 0001200, loss: 0.55702, accuracy: 0.83000\n",
      "training step: 0001300, loss: 0.85254, accuracy: 0.89000\n",
      "training step: 0001400, loss: 0.55461, accuracy: 0.86000\n",
      "training step: 0001500, loss: 0.55322, accuracy: 0.92000\n",
      "training step: 0001600, loss: 0.55209, accuracy: 0.86000\n",
      "training step: 0001700, loss: 0.55097, accuracy: 0.86000\n",
      "training step: 0001800, loss: 0.54981, accuracy: 0.87000\n",
      "training step: 0001900, loss: 0.54845, accuracy: 0.92000\n",
      "training step: 0002000, loss: 0.54742, accuracy: 0.84000\n",
      "training step: 0002100, loss: 0.54606, accuracy: 0.93000\n",
      "training step: 0002200, loss: 0.54493, accuracy: 0.87000\n",
      "training step: 0002300, loss: 0.86882, accuracy: 0.87000\n",
      "training step: 0002400, loss: 0.54262, accuracy: 0.89000\n",
      "training step: 0002500, loss: 0.54139, accuracy: 0.90000\n",
      "training step: 0002600, loss: 0.54030, accuracy: 0.87000\n",
      "training step: 0002700, loss: 0.53894, accuracy: 0.94000\n",
      "training step: 0002800, loss: 0.53778, accuracy: 0.89000\n",
      "training step: 0002900, loss: 0.53673, accuracy: 0.86000\n",
      "training step: 0003000, loss: 0.53552, accuracy: 0.91000\n",
      "training step: 0003100, loss: 0.53431, accuracy: 0.91000\n",
      "training step: 0003200, loss: 0.53321, accuracy: 0.88000\n",
      "training step: 0003300, loss: 0.53220, accuracy: 0.86000\n",
      "epoch=3\n",
      "training step: 0000100, loss: 0.53070, accuracy: 0.89000\n",
      "training step: 0000200, loss: 0.52946, accuracy: 0.93000\n",
      "training step: 0000300, loss: 0.52859, accuracy: 0.83000\n",
      "training step: 0000400, loss: 0.52758, accuracy: 0.87000\n",
      "training step: 0000500, loss: 0.52641, accuracy: 0.91000\n",
      "training step: 0000600, loss: 0.52542, accuracy: 0.87000\n",
      "training step: 0000700, loss: 0.89631, accuracy: 0.88000\n",
      "training step: 0000800, loss: 0.52349, accuracy: 0.85000\n",
      "training step: 0000900, loss: 0.52237, accuracy: 0.91000\n",
      "training step: 0001000, loss: 0.52129, accuracy: 0.90000\n",
      "training step: 0001100, loss: 0.52019, accuracy: 0.91000\n",
      "training step: 0001200, loss: 0.51910, accuracy: 0.91000\n",
      "training step: 0001300, loss: 0.51790, accuracy: 0.94000\n",
      "training step: 0001400, loss: 0.51696, accuracy: 0.87000\n",
      "training step: 0001500, loss: 0.51593, accuracy: 0.90000\n",
      "training step: 0001600, loss: 0.51489, accuracy: 0.90000\n",
      "training step: 0001700, loss: 0.91169, accuracy: 0.88000\n",
      "training step: 0001800, loss: 0.51310, accuracy: 0.85000\n",
      "training step: 0001900, loss: 0.51211, accuracy: 0.89000\n",
      "training step: 0002000, loss: 0.51118, accuracy: 0.88000\n",
      "training step: 0002100, loss: 0.51036, accuracy: 0.85000\n",
      "training step: 0002200, loss: 0.50933, accuracy: 0.91000\n",
      "training step: 0002300, loss: 0.50841, accuracy: 0.88000\n",
      "training step: 0002400, loss: 0.50749, accuracy: 0.88000\n",
      "training step: 0002500, loss: 0.50655, accuracy: 0.89000\n",
      "training step: 0002600, loss: 0.50571, accuracy: 0.86000\n",
      "training step: 0002700, loss: 0.50478, accuracy: 0.89000\n",
      "training step: 0002800, loss: 0.50389, accuracy: 0.88000\n",
      "training step: 0002900, loss: 0.50304, accuracy: 0.87000\n",
      "training step: 0003000, loss: 0.50229, accuracy: 0.84000\n",
      "training step: 0003100, loss: 0.50135, accuracy: 0.90000\n",
      "training step: 0003200, loss: 0.50048, accuracy: 0.88000\n",
      "training step: 0003300, loss: 0.49965, accuracy: 0.87000\n",
      "epoch=4\n",
      "training step: 0000100, loss: 0.49825, accuracy: 0.91000\n",
      "training step: 0000200, loss: 0.93687, accuracy: 0.89000\n",
      "training step: 0000300, loss: 0.49655, accuracy: 0.87000\n",
      "training step: 0000400, loss: 0.49578, accuracy: 0.86000\n",
      "training step: 0000500, loss: 0.94089, accuracy: 0.93000\n",
      "training step: 0000600, loss: 0.49386, accuracy: 0.91000\n",
      "training step: 0000700, loss: 0.49314, accuracy: 0.85000\n",
      "training step: 0000800, loss: 0.49220, accuracy: 0.92000\n",
      "training step: 0000900, loss: 0.49138, accuracy: 0.88000\n",
      "training step: 0001000, loss: 0.49058, accuracy: 0.88000\n",
      "training step: 0001100, loss: 0.48987, accuracy: 0.85000\n",
      "training step: 0001200, loss: 0.48907, accuracy: 0.88000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training step: 0001300, loss: 0.95070, accuracy: 0.79000\n",
      "training step: 0001400, loss: 0.48776, accuracy: 0.88000\n",
      "training step: 0001500, loss: 0.48705, accuracy: 0.86000\n",
      "training step: 0001600, loss: 0.48621, accuracy: 0.90000\n",
      "training step: 0001700, loss: 0.48534, accuracy: 0.91000\n",
      "training step: 0001800, loss: 0.48451, accuracy: 0.90000\n",
      "training step: 0001900, loss: 0.48375, accuracy: 0.88000\n",
      "training step: 0002000, loss: 0.48305, accuracy: 0.86000\n",
      "training step: 0002100, loss: 0.48224, accuracy: 0.90000\n",
      "training step: 0002200, loss: 0.48134, accuracy: 0.93000\n",
      "training step: 0002300, loss: 0.48058, accuracy: 0.88000\n",
      "training step: 0002400, loss: 0.47986, accuracy: 0.88000\n",
      "training step: 0002500, loss: 0.47915, accuracy: 0.87000\n",
      "training step: 0002600, loss: 0.47839, accuracy: 0.89000\n",
      "training step: 0002700, loss: 0.47760, accuracy: 0.90000\n",
      "training step: 0002800, loss: 0.47691, accuracy: 0.87000\n",
      "training step: 0002900, loss: 0.47620, accuracy: 0.88000\n",
      "training step: 0003000, loss: 0.47551, accuracy: 0.87000\n",
      "training step: 0003100, loss: 0.47477, accuracy: 0.89000\n",
      "training step: 0003200, loss: 0.47407, accuracy: 0.88000\n",
      "training step: 0003300, loss: 0.47328, accuracy: 0.91000\n",
      "epoch=5\n",
      "training step: 0000100, loss: 0.47208, accuracy: 0.92000\n",
      "training step: 0000200, loss: 0.47136, accuracy: 0.89000\n",
      "training step: 0000300, loss: 0.47066, accuracy: 0.88000\n",
      "training step: 0000400, loss: 0.46999, accuracy: 0.88000\n",
      "training step: 0000500, loss: 0.46941, accuracy: 0.84000\n",
      "training step: 0000600, loss: 0.46877, accuracy: 0.87000\n",
      "training step: 0000700, loss: 0.46811, accuracy: 0.88000\n",
      "training step: 0000800, loss: 0.46744, accuracy: 0.88000\n",
      "training step: 0000900, loss: 0.46672, accuracy: 0.90000\n",
      "training step: 0001000, loss: 0.46612, accuracy: 0.86000\n",
      "training step: 0001100, loss: 0.46544, accuracy: 0.89000\n",
      "training step: 0001200, loss: 0.46468, accuracy: 0.92000\n",
      "training step: 0001300, loss: 0.46409, accuracy: 0.86000\n",
      "training step: 0001400, loss: 0.46342, accuracy: 0.89000\n",
      "training step: 0001500, loss: 0.46287, accuracy: 0.85000\n",
      "training step: 0001600, loss: 0.46223, accuracy: 0.88000\n",
      "training step: 0001700, loss: 0.46161, accuracy: 0.88000\n",
      "training step: 0001800, loss: 0.46101, accuracy: 0.87000\n",
      "training step: 0001900, loss: 0.46042, accuracy: 0.87000\n",
      "training step: 0002000, loss: 0.45980, accuracy: 0.88000\n",
      "training step: 0002100, loss: 0.45910, accuracy: 0.91000\n",
      "training step: 0002200, loss: 0.45855, accuracy: 0.86000\n",
      "training step: 0002300, loss: 0.45798, accuracy: 0.87000\n",
      "training step: 0002400, loss: 0.45732, accuracy: 0.90000\n",
      "training step: 0002500, loss: 0.45670, accuracy: 0.89000\n",
      "training step: 0002600, loss: 0.45606, accuracy: 0.90000\n",
      "training step: 0002700, loss: 0.45539, accuracy: 0.91000\n",
      "training step: 0002800, loss: 0.45473, accuracy: 0.91000\n",
      "training step: 0002900, loss: 0.45414, accuracy: 0.88000\n",
      "training step: 0003000, loss: 0.45362, accuracy: 0.86000\n",
      "training step: 0003100, loss: 0.45307, accuracy: 0.87000\n",
      "training step: 0003200, loss: 0.45245, accuracy: 0.90000\n",
      "training step: 0003300, loss: 0.45181, accuracy: 0.91000\n",
      "epoch=6\n",
      "training step: 0000100, loss: 0.45090, accuracy: 0.89000\n",
      "training step: 0000200, loss: 0.45022, accuracy: 0.93000\n",
      "training step: 0000300, loss: 0.44961, accuracy: 0.90000\n",
      "training step: 0000400, loss: 0.44901, accuracy: 0.90000\n",
      "training step: 0000500, loss: 0.44845, accuracy: 0.88000\n",
      "training step: 0000600, loss: 0.44779, accuracy: 0.93000\n",
      "training step: 0000700, loss: 0.44711, accuracy: 0.93000\n",
      "training step: 0000800, loss: 0.44653, accuracy: 0.90000\n",
      "training step: 0000900, loss: 0.44602, accuracy: 0.87000\n",
      "training step: 0001000, loss: 1.02316, accuracy: 0.89000\n",
      "training step: 0001100, loss: 0.44489, accuracy: 0.90000\n",
      "training step: 0001200, loss: 0.44429, accuracy: 0.91000\n",
      "training step: 0001300, loss: 0.44386, accuracy: 0.84000\n",
      "training step: 0001400, loss: 0.44329, accuracy: 0.90000\n",
      "training step: 0001500, loss: 0.44259, accuracy: 0.96000\n",
      "training step: 0001600, loss: 0.44200, accuracy: 0.91000\n",
      "training step: 0001700, loss: 0.44160, accuracy: 0.83000\n",
      "training step: 0001800, loss: 0.44107, accuracy: 0.89000\n",
      "training step: 0001900, loss: 0.44054, accuracy: 0.89000\n",
      "training step: 0002000, loss: 0.44015, accuracy: 0.83000\n",
      "training step: 0002100, loss: 1.03359, accuracy: 0.88000\n",
      "training step: 0002200, loss: 0.43927, accuracy: 0.83000\n",
      "training step: 0002300, loss: 0.43871, accuracy: 0.91000\n",
      "training step: 0002400, loss: 0.43819, accuracy: 0.89000\n",
      "training step: 0002500, loss: 0.43766, accuracy: 0.90000\n",
      "training step: 0002600, loss: 0.43715, accuracy: 0.89000\n",
      "training step: 0002700, loss: 0.43671, accuracy: 0.86000\n",
      "training step: 0002800, loss: 0.43623, accuracy: 0.88000\n",
      "training step: 0002900, loss: 0.43582, accuracy: 0.85000\n",
      "training step: 0003000, loss: 0.43552, accuracy: 0.80000\n",
      "training step: 0003100, loss: 1.04199, accuracy: 0.88000\n",
      "training step: 0003200, loss: 0.43458, accuracy: 0.88000\n",
      "training step: 0003300, loss: 1.04355, accuracy: 0.84000\n",
      "epoch=7\n",
      "training step: 0000100, loss: 1.04509, accuracy: 0.94000\n",
      "training step: 0000200, loss: 0.43286, accuracy: 0.90000\n",
      "training step: 0000300, loss: 0.43223, accuracy: 0.96000\n",
      "training step: 0000400, loss: 0.43182, accuracy: 0.86000\n",
      "training step: 0000500, loss: 0.43136, accuracy: 0.88000\n",
      "training step: 0000600, loss: 0.43104, accuracy: 0.82000\n",
      "training step: 0000700, loss: 0.43057, accuracy: 0.89000\n",
      "training step: 0000800, loss: 0.43004, accuracy: 0.92000\n",
      "training step: 0000900, loss: 0.42966, accuracy: 0.85000\n",
      "training step: 0001000, loss: 0.42910, accuracy: 0.94000\n",
      "training step: 0001100, loss: 0.42856, accuracy: 0.93000\n",
      "training step: 0001200, loss: 0.42811, accuracy: 0.89000\n",
      "training step: 0001300, loss: 0.42768, accuracy: 0.88000\n",
      "training step: 0001400, loss: 0.42737, accuracy: 0.82000\n",
      "training step: 0001500, loss: 0.42687, accuracy: 0.92000\n",
      "training step: 0001600, loss: 0.42640, accuracy: 0.90000\n",
      "training step: 0001700, loss: 0.42600, accuracy: 0.87000\n",
      "training step: 0001800, loss: 0.42561, accuracy: 0.87000\n",
      "training step: 0001900, loss: 0.42517, accuracy: 0.89000\n",
      "training step: 0002000, loss: 0.42478, accuracy: 0.87000\n",
      "training step: 0002100, loss: 0.42435, accuracy: 0.89000\n",
      "training step: 0002200, loss: 0.42392, accuracy: 0.89000\n",
      "training step: 0002300, loss: 0.42347, accuracy: 0.90000\n",
      "training step: 0002400, loss: 0.42296, accuracy: 0.94000\n",
      "training step: 0002500, loss: 0.42259, accuracy: 0.86000\n",
      "training step: 0002600, loss: 0.42221, accuracy: 0.87000\n",
      "training step: 0002700, loss: 0.42186, accuracy: 0.86000\n",
      "training step: 0002800, loss: 0.42148, accuracy: 0.87000\n",
      "training step: 0002900, loss: 0.42105, accuracy: 0.90000\n",
      "training step: 0003000, loss: 1.06896, accuracy: 0.89000\n",
      "training step: 0003100, loss: 0.42032, accuracy: 0.85000\n",
      "training step: 0003200, loss: 0.41999, accuracy: 0.85000\n",
      "training step: 0003300, loss: 0.41967, accuracy: 0.84000\n",
      "epoch=8\n",
      "training step: 0000100, loss: 0.41909, accuracy: 0.92000\n",
      "training step: 0000200, loss: 0.41866, accuracy: 0.91000\n",
      "training step: 0000300, loss: 0.41829, accuracy: 0.88000\n",
      "training step: 0000400, loss: 0.41789, accuracy: 0.89000\n",
      "training step: 0000500, loss: 0.41749, accuracy: 0.90000\n",
      "training step: 0000600, loss: 0.41715, accuracy: 0.86000\n",
      "training step: 0000700, loss: 0.41684, accuracy: 0.85000\n",
      "training step: 0000800, loss: 0.41648, accuracy: 0.88000\n",
      "training step: 0000900, loss: 0.41606, accuracy: 0.91000\n",
      "training step: 0001000, loss: 0.41580, accuracy: 0.82000\n",
      "training step: 0001100, loss: 0.41539, accuracy: 0.91000\n",
      "training step: 0001200, loss: 0.41496, accuracy: 0.92000\n",
      "training step: 0001300, loss: 0.41468, accuracy: 0.84000\n",
      "training step: 0001400, loss: 0.41431, accuracy: 0.89000\n",
      "training step: 0001500, loss: 0.41399, accuracy: 0.86000\n",
      "training step: 0001600, loss: 0.41357, accuracy: 0.92000\n",
      "training step: 0001700, loss: 1.08344, accuracy: 0.92000\n",
      "training step: 0001800, loss: 0.41281, accuracy: 0.88000\n",
      "training step: 0001900, loss: 0.41245, accuracy: 0.89000\n",
      "training step: 0002000, loss: 0.41209, accuracy: 0.89000\n",
      "training step: 0002100, loss: 0.41179, accuracy: 0.86000\n",
      "training step: 0002200, loss: 0.41140, accuracy: 0.91000\n",
      "training step: 0002300, loss: 0.41107, accuracy: 0.88000\n",
      "training step: 0002400, loss: 0.41077, accuracy: 0.86000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training step: 0002500, loss: 1.08875, accuracy: 0.87000\n",
      "training step: 0002600, loss: 0.41000, accuracy: 0.95000\n",
      "training step: 0002700, loss: 0.40972, accuracy: 0.85000\n",
      "training step: 0002800, loss: 0.40938, accuracy: 0.89000\n",
      "training step: 0002900, loss: 0.40911, accuracy: 0.85000\n",
      "training step: 0003000, loss: 0.40871, accuracy: 0.93000\n",
      "training step: 0003100, loss: 0.40848, accuracy: 0.82000\n",
      "training step: 0003200, loss: 0.40808, accuracy: 0.93000\n",
      "training step: 0003300, loss: 1.09406, accuracy: 0.88000\n",
      "epoch=9\n",
      "training step: 0000100, loss: 0.40739, accuracy: 0.86000\n",
      "training step: 0000200, loss: 0.40703, accuracy: 0.91000\n",
      "training step: 0000300, loss: 0.40669, accuracy: 0.90000\n",
      "training step: 0000400, loss: 0.40635, accuracy: 0.90000\n",
      "training step: 0000500, loss: 0.40599, accuracy: 0.91000\n",
      "training step: 0000600, loss: 0.40579, accuracy: 0.81000\n",
      "training step: 0000700, loss: 0.40553, accuracy: 0.86000\n",
      "training step: 0000800, loss: 0.40527, accuracy: 0.85000\n",
      "training step: 0000900, loss: 0.40492, accuracy: 0.91000\n",
      "training step: 0001000, loss: 0.40462, accuracy: 0.88000\n",
      "training step: 0001100, loss: 0.40432, accuracy: 0.88000\n",
      "training step: 0001200, loss: 0.40399, accuracy: 0.90000\n",
      "training step: 0001300, loss: 0.40368, accuracy: 0.89000\n",
      "training step: 0001400, loss: 0.40331, accuracy: 0.93000\n",
      "training step: 0001500, loss: 0.40304, accuracy: 0.87000\n",
      "training step: 0001600, loss: 0.40270, accuracy: 0.91000\n",
      "training step: 0001700, loss: 0.40237, accuracy: 0.91000\n",
      "training step: 0001800, loss: 0.40205, accuracy: 0.90000\n",
      "training step: 0001900, loss: 1.10608, accuracy: 0.88000\n",
      "training step: 0002000, loss: 0.40151, accuracy: 0.86000\n",
      "training step: 0002100, loss: 0.40123, accuracy: 0.88000\n",
      "training step: 0002200, loss: 1.10761, accuracy: 0.84000\n",
      "training step: 0002300, loss: 0.40076, accuracy: 0.86000\n",
      "training step: 0002400, loss: 1.10871, accuracy: 0.89000\n",
      "training step: 0002500, loss: 0.40019, accuracy: 0.88000\n",
      "training step: 0002600, loss: 0.39993, accuracy: 0.87000\n",
      "training step: 0002700, loss: 0.39969, accuracy: 0.86000\n",
      "training step: 0002800, loss: 0.39935, accuracy: 0.92000\n",
      "training step: 0002900, loss: 0.39908, accuracy: 0.88000\n",
      "training step: 0003000, loss: 1.11222, accuracy: 0.93000\n",
      "training step: 0003100, loss: 1.11283, accuracy: 0.90000\n",
      "training step: 0003200, loss: 0.39818, accuracy: 0.88000\n",
      "training step: 0003300, loss: 0.39792, accuracy: 0.88000\n",
      "epoch=10\n",
      "training step: 0000100, loss: 0.39754, accuracy: 0.88000\n",
      "training step: 0000200, loss: 0.39731, accuracy: 0.86000\n",
      "training step: 0000300, loss: 0.39699, accuracy: 0.92000\n",
      "training step: 0000400, loss: 0.39668, accuracy: 0.92000\n",
      "training step: 0000500, loss: 0.39646, accuracy: 0.85000\n",
      "training step: 0000600, loss: 0.39619, accuracy: 0.89000\n",
      "training step: 0000700, loss: 0.39590, accuracy: 0.91000\n",
      "training step: 0000800, loss: 0.39569, accuracy: 0.84000\n",
      "training step: 0000900, loss: 0.39542, accuracy: 0.90000\n",
      "training step: 0001000, loss: 0.39516, accuracy: 0.88000\n",
      "training step: 0001100, loss: 0.39495, accuracy: 0.86000\n",
      "training step: 0001200, loss: 0.39466, accuracy: 0.91000\n",
      "training step: 0001300, loss: 0.39433, accuracy: 0.94000\n",
      "training step: 0001400, loss: 0.39410, accuracy: 0.87000\n",
      "training step: 0001500, loss: 0.39389, accuracy: 0.85000\n",
      "training step: 0001600, loss: 1.12261, accuracy: 0.85000\n",
      "training step: 0001700, loss: 0.39349, accuracy: 0.85000\n",
      "training step: 0001800, loss: 0.39322, accuracy: 0.90000\n",
      "training step: 0001900, loss: 0.39294, accuracy: 0.91000\n",
      "training step: 0002000, loss: 0.39267, accuracy: 0.91000\n",
      "training step: 0002100, loss: 1.12535, accuracy: 0.92000\n",
      "training step: 0002200, loss: 0.39208, accuracy: 0.93000\n",
      "training step: 0002300, loss: 0.39179, accuracy: 0.92000\n",
      "training step: 0002400, loss: 0.39156, accuracy: 0.88000\n",
      "training step: 0002500, loss: 0.39133, accuracy: 0.88000\n",
      "training step: 0002600, loss: 0.39108, accuracy: 0.89000\n",
      "training step: 0002700, loss: 0.39093, accuracy: 0.82000\n",
      "training step: 0002800, loss: 0.39067, accuracy: 0.91000\n",
      "training step: 0002900, loss: 0.39043, accuracy: 0.89000\n",
      "training step: 0003000, loss: 0.39016, accuracy: 0.91000\n",
      "training step: 0003100, loss: 0.38996, accuracy: 0.86000\n",
      "training step: 0003200, loss: 0.38980, accuracy: 0.83000\n",
      "training step: 0003300, loss: 0.38959, accuracy: 0.87000\n",
      "epoch=11\n",
      "training step: 0000100, loss: 0.38923, accuracy: 0.94000\n",
      "training step: 0000200, loss: 0.38904, accuracy: 0.85000\n",
      "training step: 0000300, loss: 0.38881, accuracy: 0.89000\n",
      "training step: 0000400, loss: 0.38861, accuracy: 0.87000\n",
      "training step: 0000500, loss: 0.38835, accuracy: 0.91000\n",
      "training step: 0000600, loss: 0.38816, accuracy: 0.86000\n",
      "training step: 0000700, loss: 0.38791, accuracy: 0.91000\n",
      "training step: 0000800, loss: 0.38767, accuracy: 0.90000\n",
      "training step: 0000900, loss: 0.38750, accuracy: 0.85000\n",
      "training step: 0001000, loss: 0.38725, accuracy: 0.91000\n",
      "training step: 0001100, loss: 0.38705, accuracy: 0.87000\n",
      "training step: 0001200, loss: 0.38683, accuracy: 0.89000\n",
      "training step: 0001300, loss: 0.38659, accuracy: 0.91000\n",
      "training step: 0001400, loss: 1.13796, accuracy: 0.89000\n",
      "training step: 0001500, loss: 0.38613, accuracy: 0.91000\n",
      "training step: 0001600, loss: 0.38591, accuracy: 0.89000\n",
      "training step: 0001700, loss: 0.38569, accuracy: 0.89000\n",
      "training step: 0001800, loss: 0.38556, accuracy: 0.82000\n",
      "training step: 0001900, loss: 0.38538, accuracy: 0.86000\n",
      "training step: 0002000, loss: 0.38514, accuracy: 0.92000\n",
      "training step: 0002100, loss: 0.38488, accuracy: 0.93000\n",
      "training step: 0002200, loss: 0.38472, accuracy: 0.84000\n",
      "training step: 0002300, loss: 0.38458, accuracy: 0.83000\n",
      "training step: 0002400, loss: 0.38441, accuracy: 0.86000\n",
      "training step: 0002500, loss: 0.38424, accuracy: 0.86000\n",
      "training step: 0002600, loss: 0.38403, accuracy: 0.89000\n",
      "training step: 0002700, loss: 0.38383, accuracy: 0.88000\n",
      "training step: 0002800, loss: 0.38364, accuracy: 0.88000\n",
      "training step: 0002900, loss: 0.38347, accuracy: 0.86000\n",
      "training step: 0003000, loss: 0.38322, accuracy: 0.93000\n",
      "training step: 0003100, loss: 0.38303, accuracy: 0.88000\n",
      "training step: 0003200, loss: 0.38283, accuracy: 0.89000\n",
      "training step: 0003300, loss: 0.38261, accuracy: 0.91000\n",
      "epoch=12\n",
      "training step: 0000100, loss: 1.14656, accuracy: 0.87000\n",
      "training step: 0000200, loss: 0.38209, accuracy: 0.93000\n",
      "training step: 0000300, loss: 0.38191, accuracy: 0.88000\n",
      "training step: 0000400, loss: 1.14786, accuracy: 0.87000\n",
      "training step: 0000500, loss: 0.38151, accuracy: 0.92000\n",
      "training step: 0000600, loss: 0.38133, accuracy: 0.87000\n",
      "training step: 0000700, loss: 0.38115, accuracy: 0.88000\n",
      "training step: 0000800, loss: 0.38096, accuracy: 0.89000\n",
      "training step: 0000900, loss: 0.38077, accuracy: 0.89000\n",
      "training step: 0001000, loss: 0.38060, accuracy: 0.87000\n",
      "training step: 0001100, loss: 0.38041, accuracy: 0.89000\n",
      "training step: 0001200, loss: 0.38028, accuracy: 0.84000\n",
      "training step: 0001300, loss: 0.38012, accuracy: 0.86000\n",
      "training step: 0001400, loss: 0.37994, accuracy: 0.88000\n",
      "training step: 0001500, loss: 0.37979, accuracy: 0.85000\n",
      "training step: 0001600, loss: 0.37964, accuracy: 0.86000\n",
      "training step: 0001700, loss: 0.37948, accuracy: 0.87000\n",
      "training step: 0001800, loss: 0.37927, accuracy: 0.91000\n",
      "training step: 0001900, loss: 0.37908, accuracy: 0.90000\n",
      "training step: 0002000, loss: 0.37889, accuracy: 0.90000\n",
      "training step: 0002100, loss: 0.37873, accuracy: 0.87000\n",
      "training step: 0002200, loss: 0.37852, accuracy: 0.91000\n",
      "training step: 0002300, loss: 0.37836, accuracy: 0.88000\n",
      "training step: 0002400, loss: 0.37820, accuracy: 0.87000\n",
      "training step: 0002500, loss: 0.37800, accuracy: 0.91000\n",
      "training step: 0002600, loss: 0.37781, accuracy: 0.90000\n",
      "training step: 0002700, loss: 0.37757, accuracy: 0.95000\n",
      "training step: 0002800, loss: 1.15718, accuracy: 0.86000\n",
      "training step: 0002900, loss: 0.37726, accuracy: 0.88000\n",
      "training step: 0003000, loss: 0.37708, accuracy: 0.90000\n",
      "training step: 0003100, loss: 0.37692, accuracy: 0.88000\n",
      "training step: 0003200, loss: 0.37676, accuracy: 0.88000\n",
      "training step: 0003300, loss: 0.37662, accuracy: 0.85000\n",
      "epoch=13\n",
      "training step: 0000100, loss: 0.37634, accuracy: 0.91000\n",
      "training step: 0000200, loss: 0.37616, accuracy: 0.90000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training step: 0000300, loss: 0.37602, accuracy: 0.87000\n",
      "training step: 0000400, loss: 0.37589, accuracy: 0.85000\n",
      "training step: 0000500, loss: 0.37577, accuracy: 0.84000\n",
      "training step: 0000600, loss: 1.16110, accuracy: 0.85000\n",
      "training step: 0000700, loss: 0.37550, accuracy: 0.86000\n",
      "training step: 0000800, loss: 0.37536, accuracy: 0.86000\n",
      "training step: 0000900, loss: 0.37525, accuracy: 0.84000\n",
      "training step: 0001000, loss: 0.37510, accuracy: 0.87000\n",
      "training step: 0001100, loss: 0.37495, accuracy: 0.87000\n",
      "training step: 0001200, loss: 0.37479, accuracy: 0.89000\n",
      "training step: 0001300, loss: 1.16331, accuracy: 0.89000\n",
      "training step: 0001400, loss: 0.37443, accuracy: 0.93000\n",
      "training step: 0001500, loss: 0.37423, accuracy: 0.93000\n",
      "training step: 0001600, loss: 0.37408, accuracy: 0.88000\n",
      "training step: 0001700, loss: 0.37392, accuracy: 0.89000\n",
      "training step: 0001800, loss: 0.37375, accuracy: 0.91000\n",
      "training step: 0001900, loss: 0.37359, accuracy: 0.89000\n",
      "training step: 0002000, loss: 1.16592, accuracy: 0.87000\n",
      "training step: 0002100, loss: 1.16622, accuracy: 0.87000\n",
      "training step: 0002200, loss: 0.37314, accuracy: 0.91000\n",
      "training step: 0002300, loss: 1.16695, accuracy: 0.89000\n",
      "training step: 0002400, loss: 0.37282, accuracy: 0.90000\n",
      "training step: 0002500, loss: 0.37267, accuracy: 0.88000\n",
      "training step: 0002600, loss: 0.37253, accuracy: 0.88000\n",
      "training step: 0002700, loss: 0.37239, accuracy: 0.88000\n",
      "training step: 0002800, loss: 0.37220, accuracy: 0.93000\n",
      "training step: 0002900, loss: 0.37207, accuracy: 0.87000\n",
      "training step: 0003000, loss: 0.37195, accuracy: 0.85000\n",
      "training step: 0003100, loss: 0.37178, accuracy: 0.91000\n",
      "training step: 0003200, loss: 0.37162, accuracy: 0.90000\n",
      "training step: 0003300, loss: 0.37146, accuracy: 0.91000\n",
      "epoch=14\n",
      "training step: 0000100, loss: 0.37122, accuracy: 0.92000\n",
      "training step: 0000200, loss: 0.37102, accuracy: 0.95000\n",
      "training step: 0000300, loss: 0.37089, accuracy: 0.87000\n",
      "training step: 0000400, loss: 1.17193, accuracy: 0.90000\n",
      "training step: 0000500, loss: 0.37064, accuracy: 0.84000\n",
      "training step: 0000600, loss: 0.37047, accuracy: 0.92000\n",
      "training step: 0000700, loss: 0.37032, accuracy: 0.90000\n",
      "training step: 0000800, loss: 0.37020, accuracy: 0.86000\n",
      "training step: 0000900, loss: 0.37008, accuracy: 0.86000\n",
      "training step: 0001000, loss: 0.36992, accuracy: 0.92000\n",
      "training step: 0001100, loss: 1.17408, accuracy: 0.89000\n",
      "training step: 0001200, loss: 0.36966, accuracy: 0.86000\n",
      "training step: 0001300, loss: 0.36956, accuracy: 0.84000\n",
      "training step: 0001400, loss: 0.36938, accuracy: 0.94000\n",
      "training step: 0001500, loss: 0.36926, accuracy: 0.87000\n",
      "training step: 0001600, loss: 0.36913, accuracy: 0.88000\n",
      "training step: 0001700, loss: 0.36901, accuracy: 0.87000\n",
      "training step: 0001800, loss: 0.36892, accuracy: 0.83000\n",
      "training step: 0001900, loss: 0.36879, accuracy: 0.88000\n",
      "training step: 0002000, loss: 0.36865, accuracy: 0.90000\n",
      "training step: 0002100, loss: 0.36856, accuracy: 0.83000\n",
      "training step: 0002200, loss: 0.36841, accuracy: 0.92000\n",
      "training step: 0002300, loss: 0.36829, accuracy: 0.87000\n",
      "training step: 0002400, loss: 1.17758, accuracy: 0.82000\n",
      "training step: 0002500, loss: 0.36803, accuracy: 0.95000\n",
      "training step: 0002600, loss: 0.36791, accuracy: 0.88000\n",
      "training step: 0002700, loss: 0.36776, accuracy: 0.91000\n",
      "training step: 0002800, loss: 0.36765, accuracy: 0.86000\n",
      "training step: 0002900, loss: 0.36747, accuracy: 0.95000\n",
      "training step: 0003000, loss: 0.36735, accuracy: 0.88000\n",
      "training step: 0003100, loss: 1.17969, accuracy: 0.82000\n",
      "training step: 0003200, loss: 0.36718, accuracy: 0.85000\n",
      "training step: 0003300, loss: 0.36700, accuracy: 0.95000\n",
      "epoch=15\n",
      "training step: 0000100, loss: 0.36679, accuracy: 0.93000\n",
      "training step: 0000200, loss: 0.36661, accuracy: 0.95000\n",
      "training step: 0000300, loss: 0.36646, accuracy: 0.92000\n",
      "training step: 0000400, loss: 0.36632, accuracy: 0.91000\n",
      "training step: 0000500, loss: 0.36618, accuracy: 0.91000\n",
      "training step: 0000600, loss: 0.36604, accuracy: 0.91000\n",
      "training step: 0000700, loss: 0.36592, accuracy: 0.88000\n",
      "training step: 0000800, loss: 0.36582, accuracy: 0.87000\n",
      "training step: 0000900, loss: 0.36571, accuracy: 0.87000\n",
      "training step: 0001000, loss: 0.36558, accuracy: 0.90000\n",
      "training step: 0001100, loss: 0.36543, accuracy: 0.92000\n",
      "training step: 0001200, loss: 0.36534, accuracy: 0.85000\n",
      "training step: 0001300, loss: 0.36521, accuracy: 0.91000\n",
      "training step: 0001400, loss: 0.36511, accuracy: 0.86000\n",
      "training step: 0001500, loss: 1.18477, accuracy: 0.84000\n",
      "training step: 0001600, loss: 0.36489, accuracy: 0.91000\n",
      "training step: 0001700, loss: 1.18533, accuracy: 0.88000\n",
      "training step: 0001800, loss: 1.18559, accuracy: 0.88000\n",
      "training step: 0001900, loss: 0.36456, accuracy: 0.87000\n",
      "training step: 0002000, loss: 0.36445, accuracy: 0.88000\n",
      "training step: 0002100, loss: 0.36431, accuracy: 0.92000\n",
      "training step: 0002200, loss: 0.36426, accuracy: 0.81000\n",
      "training step: 0002300, loss: 1.18667, accuracy: 0.82000\n",
      "training step: 0002400, loss: 0.36410, accuracy: 0.86000\n",
      "training step: 0002500, loss: 1.18708, accuracy: 0.85000\n",
      "training step: 0002600, loss: 0.36389, accuracy: 0.90000\n",
      "training step: 0002700, loss: 0.36376, accuracy: 0.91000\n",
      "training step: 0002800, loss: 0.36364, accuracy: 0.90000\n",
      "training step: 0002900, loss: 0.36355, accuracy: 0.85000\n",
      "training step: 0003000, loss: 1.18832, accuracy: 0.85000\n",
      "training step: 0003100, loss: 0.36335, accuracy: 0.89000\n",
      "training step: 0003200, loss: 0.36327, accuracy: 0.85000\n",
      "training step: 0003300, loss: 0.36313, accuracy: 0.92000\n",
      "epoch=16\n",
      "training step: 0000100, loss: 0.36300, accuracy: 0.85000\n",
      "training step: 0000200, loss: 0.36291, accuracy: 0.86000\n",
      "training step: 0000300, loss: 0.36280, accuracy: 0.88000\n",
      "training step: 0000400, loss: 1.19015, accuracy: 0.93000\n",
      "training step: 0000500, loss: 0.36256, accuracy: 0.89000\n",
      "training step: 0000600, loss: 1.19068, accuracy: 0.91000\n",
      "training step: 0000700, loss: 0.36234, accuracy: 0.87000\n",
      "training step: 0000800, loss: 0.36224, accuracy: 0.87000\n",
      "training step: 0000900, loss: 0.36211, accuracy: 0.92000\n",
      "training step: 0001000, loss: 0.36203, accuracy: 0.86000\n",
      "training step: 0001100, loss: 0.36192, accuracy: 0.88000\n",
      "training step: 0001200, loss: 0.36181, accuracy: 0.90000\n",
      "training step: 0001300, loss: 0.36170, accuracy: 0.90000\n",
      "training step: 0001400, loss: 1.19258, accuracy: 0.87000\n",
      "training step: 0001500, loss: 0.36151, accuracy: 0.87000\n",
      "training step: 0001600, loss: 0.36140, accuracy: 0.90000\n",
      "training step: 0001700, loss: 0.36133, accuracy: 0.83000\n",
      "training step: 0001800, loss: 0.36122, accuracy: 0.91000\n",
      "training step: 0001900, loss: 0.36107, accuracy: 0.95000\n",
      "training step: 0002000, loss: 0.36100, accuracy: 0.85000\n",
      "training step: 0002100, loss: 1.19417, accuracy: 0.86000\n",
      "training step: 0002200, loss: 0.36076, accuracy: 0.96000\n",
      "training step: 0002300, loss: 0.36068, accuracy: 0.86000\n",
      "training step: 0002400, loss: 0.36058, accuracy: 0.88000\n",
      "training step: 0002500, loss: 0.36051, accuracy: 0.84000\n",
      "training step: 0002600, loss: 0.36044, accuracy: 0.85000\n",
      "training step: 0002700, loss: 0.36033, accuracy: 0.90000\n",
      "training step: 0002800, loss: 0.36022, accuracy: 0.91000\n",
      "training step: 0002900, loss: 0.36010, accuracy: 0.91000\n",
      "training step: 0003000, loss: 0.36000, accuracy: 0.90000\n",
      "training step: 0003100, loss: 0.35992, accuracy: 0.86000\n",
      "training step: 0003200, loss: 0.35984, accuracy: 0.86000\n",
      "training step: 0003300, loss: 0.35974, accuracy: 0.89000\n",
      "epoch=17\n",
      "training step: 0000100, loss: 0.35961, accuracy: 0.86000\n",
      "training step: 0000200, loss: 0.35950, accuracy: 0.91000\n",
      "training step: 0000300, loss: 0.35943, accuracy: 0.86000\n",
      "training step: 0000400, loss: 0.35932, accuracy: 0.90000\n",
      "training step: 0000500, loss: 0.35925, accuracy: 0.85000\n",
      "training step: 0000600, loss: 0.35916, accuracy: 0.88000\n",
      "training step: 0000700, loss: 0.35908, accuracy: 0.87000\n",
      "training step: 0000800, loss: 0.35903, accuracy: 0.81000\n",
      "training step: 0000900, loss: 0.35896, accuracy: 0.86000\n",
      "training step: 0001000, loss: 1.19884, accuracy: 0.84000\n",
      "training step: 0001100, loss: 0.35878, accuracy: 0.92000\n",
      "training step: 0001200, loss: 0.35869, accuracy: 0.88000\n",
      "training step: 0001300, loss: 0.35859, accuracy: 0.89000\n",
      "training step: 0001400, loss: 0.35846, accuracy: 0.95000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training step: 0001500, loss: 0.35841, accuracy: 0.82000\n",
      "training step: 0001600, loss: 0.35833, accuracy: 0.87000\n",
      "training step: 0001700, loss: 0.35824, accuracy: 0.89000\n",
      "training step: 0001800, loss: 0.35815, accuracy: 0.88000\n",
      "training step: 0001900, loss: 1.20082, accuracy: 0.93000\n",
      "training step: 0002000, loss: 0.35794, accuracy: 0.90000\n",
      "training step: 0002100, loss: 0.35782, accuracy: 0.93000\n",
      "training step: 0002200, loss: 0.35773, accuracy: 0.89000\n",
      "training step: 0002300, loss: 0.35764, accuracy: 0.88000\n",
      "training step: 0002400, loss: 0.35756, accuracy: 0.88000\n",
      "training step: 0002500, loss: 0.35748, accuracy: 0.88000\n",
      "training step: 0002600, loss: 0.35736, accuracy: 0.93000\n",
      "training step: 0002700, loss: 0.35724, accuracy: 0.94000\n",
      "training step: 0002800, loss: 1.20282, accuracy: 0.85000\n",
      "training step: 0002900, loss: 0.35710, accuracy: 0.86000\n",
      "training step: 0003000, loss: 1.20317, accuracy: 0.88000\n",
      "training step: 0003100, loss: 0.35694, accuracy: 0.89000\n",
      "training step: 0003200, loss: 0.35686, accuracy: 0.87000\n",
      "training step: 0003300, loss: 0.35676, accuracy: 0.91000\n",
      "epoch=18\n",
      "training step: 0000100, loss: 0.35658, accuracy: 0.95000\n",
      "training step: 0000200, loss: 1.20440, accuracy: 0.89000\n",
      "training step: 0000300, loss: 0.35641, accuracy: 0.89000\n",
      "training step: 0000400, loss: 0.35634, accuracy: 0.86000\n",
      "training step: 0000500, loss: 0.35625, accuracy: 0.90000\n",
      "training step: 0000600, loss: 0.35615, accuracy: 0.91000\n",
      "training step: 0000700, loss: 0.35606, accuracy: 0.91000\n",
      "training step: 0000800, loss: 0.35596, accuracy: 0.91000\n",
      "training step: 0000900, loss: 1.20586, accuracy: 0.90000\n",
      "training step: 0001000, loss: 0.35580, accuracy: 0.86000\n",
      "training step: 0001100, loss: 0.35571, accuracy: 0.91000\n",
      "training step: 0001200, loss: 0.35563, accuracy: 0.88000\n",
      "training step: 0001300, loss: 1.20659, accuracy: 0.87000\n",
      "training step: 0001400, loss: 0.35548, accuracy: 0.89000\n",
      "training step: 0001500, loss: 0.35541, accuracy: 0.87000\n",
      "training step: 0001600, loss: 0.35532, accuracy: 0.90000\n",
      "training step: 0001700, loss: 0.35524, accuracy: 0.89000\n",
      "training step: 0001800, loss: 0.35519, accuracy: 0.83000\n",
      "training step: 0001900, loss: 0.35509, accuracy: 0.92000\n",
      "training step: 0002000, loss: 0.35500, accuracy: 0.91000\n",
      "training step: 0002100, loss: 0.35493, accuracy: 0.86000\n",
      "training step: 0002200, loss: 0.35491, accuracy: 0.78000\n",
      "training step: 0002300, loss: 0.35484, accuracy: 0.88000\n",
      "training step: 0002400, loss: 0.35477, accuracy: 0.88000\n",
      "training step: 0002500, loss: 0.35467, accuracy: 0.92000\n",
      "training step: 0002600, loss: 0.35463, accuracy: 0.82000\n",
      "training step: 0002700, loss: 0.35457, accuracy: 0.86000\n",
      "training step: 0002800, loss: 0.35449, accuracy: 0.89000\n",
      "training step: 0002900, loss: 0.35441, accuracy: 0.89000\n",
      "training step: 0003000, loss: 0.35432, accuracy: 0.90000\n",
      "training step: 0003100, loss: 0.35423, accuracy: 0.92000\n",
      "training step: 0003200, loss: 0.35415, accuracy: 0.89000\n",
      "training step: 0003300, loss: 0.35409, accuracy: 0.86000\n",
      "epoch=19\n",
      "training step: 0000100, loss: 0.35400, accuracy: 0.87000\n",
      "training step: 0000200, loss: 0.35393, accuracy: 0.87000\n",
      "training step: 0000300, loss: 0.35383, accuracy: 0.93000\n",
      "training step: 0000400, loss: 0.35376, accuracy: 0.89000\n",
      "training step: 0000500, loss: 0.35367, accuracy: 0.91000\n",
      "training step: 0000600, loss: 0.35358, accuracy: 0.91000\n",
      "training step: 0000700, loss: 0.35350, accuracy: 0.91000\n",
      "training step: 0000800, loss: 0.35346, accuracy: 0.82000\n",
      "training step: 0000900, loss: 0.35339, accuracy: 0.87000\n",
      "training step: 0001000, loss: 0.35331, accuracy: 0.90000\n",
      "training step: 0001100, loss: 0.35326, accuracy: 0.85000\n",
      "training step: 0001200, loss: 0.35320, accuracy: 0.87000\n",
      "training step: 0001300, loss: 0.35311, accuracy: 0.91000\n",
      "training step: 0001400, loss: 0.35303, accuracy: 0.91000\n",
      "training step: 0001500, loss: 0.35296, accuracy: 0.87000\n",
      "training step: 0001600, loss: 0.35289, accuracy: 0.90000\n",
      "training step: 0001700, loss: 0.35281, accuracy: 0.89000\n",
      "training step: 0001800, loss: 0.35272, accuracy: 0.93000\n",
      "training step: 0001900, loss: 0.35265, accuracy: 0.89000\n",
      "training step: 0002000, loss: 0.35257, accuracy: 0.89000\n",
      "training step: 0002100, loss: 0.35251, accuracy: 0.88000\n",
      "training step: 0002200, loss: 1.21391, accuracy: 0.86000\n",
      "training step: 0002300, loss: 0.35241, accuracy: 0.83000\n",
      "training step: 0002400, loss: 0.35233, accuracy: 0.90000\n",
      "training step: 0002500, loss: 0.35225, accuracy: 0.92000\n",
      "training step: 0002600, loss: 0.35219, accuracy: 0.87000\n",
      "training step: 0002700, loss: 0.35212, accuracy: 0.89000\n",
      "training step: 0002800, loss: 0.35204, accuracy: 0.91000\n",
      "training step: 0002900, loss: 0.35196, accuracy: 0.91000\n",
      "training step: 0003000, loss: 0.35190, accuracy: 0.87000\n",
      "training step: 0003100, loss: 1.21538, accuracy: 0.88000\n",
      "training step: 0003200, loss: 0.35177, accuracy: 0.88000\n",
      "training step: 0003300, loss: 0.35173, accuracy: 0.82000\n"
     ]
    }
   ],
   "source": [
    "in_dim, hidden_dim, out_dim = 1024, 128, 2\n",
    "model = model_init(in_dim, hidden_dim, out_dim, 40) # originally 20\n",
    "optim, criterion = optim_init(model, 1.0e-4, 1.0e-2)\n",
    "for epoch in range(20): # Originally 20\n",
    "    print(f\"{epoch=}\")\n",
    "    dataloader = example_loader(data_iter(\"DATA/jsons/training_data.json.gz\"), in_dim)\n",
    "    model = train(model, optim, criterion, dataloader, interval=100, converge_limit=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "813ed97b",
   "metadata": {},
   "source": [
    "# Saving model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "f57f36b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, \"./model/trained_model.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb34ef5-cc27-4940-9c2d-78eae4126a98",
   "metadata": {},
   "source": [
    "# Model evaluation\n",
    "\n",
    "Here we visualise some evaluation examples (that the model has not seen), and compare the prediction to the target.\n",
    "\n",
    "Note: evaluation should be done using metrics like accuracy, confusion matrices, _etc._ over the whole validation set.  We look at one example at a time as a sanity check when prototyping, but this is only a preliminary step!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b986ab",
   "metadata": {},
   "source": [
    "# Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "8a09feeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load(\"./model/trained_model.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "79a60109-2aeb-4857-ac1c-4dc14c2bd889",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "dataloader = example_loader(data_iter(\"./DATA/jsons/training_data.json.gz\"), in_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7f00fdf6-7f34-410e-b2cd-b6275a43343f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction: 0, target: 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "hovertemplate": "x=%{x}<br>y=%{y}<extra></extra>",
         "legendgroup": "",
         "line": {
          "color": "#636efa",
          "dash": "solid"
         },
         "marker": {
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "",
         "showlegend": false,
         "type": "scattergl",
         "x": [
          0.01220703125,
          0.03662109375,
          0.06103515625,
          0.08544921875,
          0.10986328125,
          0.13427734375,
          0.15869140625,
          0.18310546875,
          0.20751953125,
          0.23193359375,
          0.25634765625,
          0.28076171875,
          0.30517578125,
          0.32958984375,
          0.35400390625,
          0.37841796875,
          0.40283203125,
          0.42724609375,
          0.45166015625,
          0.47607421875,
          0.50048828125,
          0.52490234375,
          0.54931640625,
          0.57373046875,
          0.59814453125,
          0.62255859375,
          0.64697265625,
          0.67138671875,
          0.69580078125,
          0.72021484375,
          0.74462890625,
          0.76904296875,
          0.79345703125,
          0.81787109375,
          0.84228515625,
          0.86669921875,
          0.89111328125,
          0.91552734375,
          0.93994140625,
          0.96435546875,
          0.98876953125,
          1.01318359375,
          1.03759765625,
          1.06201171875,
          1.08642578125,
          1.11083984375,
          1.13525390625,
          1.15966796875,
          1.18408203125,
          1.20849609375,
          1.23291015625,
          1.25732421875,
          1.28173828125,
          1.30615234375,
          1.33056640625,
          1.35498046875,
          1.37939453125,
          1.40380859375,
          1.42822265625,
          1.45263671875,
          1.47705078125,
          1.50146484375,
          1.52587890625,
          1.55029296875,
          1.57470703125,
          1.59912109375,
          1.62353515625,
          1.64794921875,
          1.67236328125,
          1.69677734375,
          1.72119140625,
          1.74560546875,
          1.77001953125,
          1.79443359375,
          1.81884765625,
          1.84326171875,
          1.86767578125,
          1.89208984375,
          1.91650390625,
          1.94091796875,
          1.96533203125,
          1.98974609375,
          2.01416015625,
          2.03857421875,
          2.06298828125,
          2.08740234375,
          2.11181640625,
          2.13623046875,
          2.16064453125,
          2.18505859375,
          2.20947265625,
          2.23388671875,
          2.25830078125,
          2.28271484375,
          2.30712890625,
          2.33154296875,
          2.35595703125,
          2.38037109375,
          2.40478515625,
          2.42919921875,
          2.45361328125,
          2.47802734375,
          2.50244140625,
          2.52685546875,
          2.55126953125,
          2.57568359375,
          2.60009765625,
          2.62451171875,
          2.64892578125,
          2.67333984375,
          2.69775390625,
          2.72216796875,
          2.74658203125,
          2.77099609375,
          2.79541015625,
          2.81982421875,
          2.84423828125,
          2.86865234375,
          2.89306640625,
          2.91748046875,
          2.94189453125,
          2.96630859375,
          2.99072265625,
          3.01513671875,
          3.03955078125,
          3.06396484375,
          3.08837890625,
          3.11279296875,
          3.13720703125,
          3.16162109375,
          3.18603515625,
          3.21044921875,
          3.23486328125,
          3.25927734375,
          3.28369140625,
          3.30810546875,
          3.33251953125,
          3.35693359375,
          3.38134765625,
          3.40576171875,
          3.43017578125,
          3.45458984375,
          3.47900390625,
          3.50341796875,
          3.52783203125,
          3.55224609375,
          3.57666015625,
          3.60107421875,
          3.62548828125,
          3.64990234375,
          3.67431640625,
          3.69873046875,
          3.72314453125,
          3.74755859375,
          3.77197265625,
          3.79638671875,
          3.82080078125,
          3.84521484375,
          3.86962890625,
          3.89404296875,
          3.91845703125,
          3.94287109375,
          3.96728515625,
          3.99169921875,
          4.01611328125,
          4.04052734375,
          4.06494140625,
          4.08935546875,
          4.11376953125,
          4.13818359375,
          4.16259765625,
          4.18701171875,
          4.21142578125,
          4.23583984375,
          4.26025390625,
          4.28466796875,
          4.30908203125,
          4.33349609375,
          4.35791015625,
          4.38232421875,
          4.40673828125,
          4.43115234375,
          4.45556640625,
          4.47998046875,
          4.50439453125,
          4.52880859375,
          4.55322265625,
          4.57763671875,
          4.60205078125,
          4.62646484375,
          4.65087890625,
          4.67529296875,
          4.69970703125,
          4.72412109375,
          4.74853515625,
          4.77294921875,
          4.79736328125,
          4.82177734375,
          4.84619140625,
          4.87060546875,
          4.89501953125,
          4.91943359375,
          4.94384765625,
          4.96826171875,
          4.99267578125,
          5.01708984375,
          5.04150390625,
          5.06591796875,
          5.09033203125,
          5.11474609375,
          5.13916015625,
          5.16357421875,
          5.18798828125,
          5.21240234375,
          5.23681640625,
          5.26123046875,
          5.28564453125,
          5.31005859375,
          5.33447265625,
          5.35888671875,
          5.38330078125,
          5.40771484375,
          5.43212890625,
          5.45654296875,
          5.48095703125,
          5.50537109375,
          5.52978515625,
          5.55419921875,
          5.57861328125,
          5.60302734375,
          5.62744140625,
          5.65185546875,
          5.67626953125,
          5.70068359375,
          5.72509765625,
          5.74951171875,
          5.77392578125,
          5.79833984375,
          5.82275390625,
          5.84716796875,
          5.87158203125,
          5.89599609375,
          5.92041015625,
          5.94482421875,
          5.96923828125,
          5.99365234375,
          6.01806640625,
          6.04248046875,
          6.06689453125,
          6.09130859375,
          6.11572265625,
          6.14013671875,
          6.16455078125,
          6.18896484375,
          6.21337890625,
          6.23779296875,
          6.26220703125,
          6.28662109375,
          6.31103515625,
          6.33544921875,
          6.35986328125,
          6.38427734375,
          6.40869140625,
          6.43310546875,
          6.45751953125,
          6.48193359375,
          6.50634765625,
          6.53076171875,
          6.55517578125,
          6.57958984375,
          6.60400390625,
          6.62841796875,
          6.65283203125,
          6.67724609375,
          6.70166015625,
          6.72607421875,
          6.75048828125,
          6.77490234375,
          6.79931640625,
          6.82373046875,
          6.84814453125,
          6.87255859375,
          6.89697265625,
          6.92138671875,
          6.94580078125,
          6.97021484375,
          6.99462890625,
          7.01904296875,
          7.04345703125,
          7.06787109375,
          7.09228515625,
          7.11669921875,
          7.14111328125,
          7.16552734375,
          7.18994140625,
          7.21435546875,
          7.23876953125,
          7.26318359375,
          7.28759765625,
          7.31201171875,
          7.33642578125,
          7.36083984375,
          7.38525390625,
          7.40966796875,
          7.43408203125,
          7.45849609375,
          7.48291015625,
          7.50732421875,
          7.53173828125,
          7.55615234375,
          7.58056640625,
          7.60498046875,
          7.62939453125,
          7.65380859375,
          7.67822265625,
          7.70263671875,
          7.72705078125,
          7.75146484375,
          7.77587890625,
          7.80029296875,
          7.82470703125,
          7.84912109375,
          7.87353515625,
          7.89794921875,
          7.92236328125,
          7.94677734375,
          7.97119140625,
          7.99560546875,
          8.02001953125,
          8.04443359375,
          8.06884765625,
          8.09326171875,
          8.11767578125,
          8.14208984375,
          8.16650390625,
          8.19091796875,
          8.21533203125,
          8.23974609375,
          8.26416015625,
          8.28857421875,
          8.31298828125,
          8.33740234375,
          8.36181640625,
          8.38623046875,
          8.41064453125,
          8.43505859375,
          8.45947265625,
          8.48388671875,
          8.50830078125,
          8.53271484375,
          8.55712890625,
          8.58154296875,
          8.60595703125,
          8.63037109375,
          8.65478515625,
          8.67919921875,
          8.70361328125,
          8.72802734375,
          8.75244140625,
          8.77685546875,
          8.80126953125,
          8.82568359375,
          8.85009765625,
          8.87451171875,
          8.89892578125,
          8.92333984375,
          8.94775390625,
          8.97216796875,
          8.99658203125,
          9.02099609375,
          9.04541015625,
          9.06982421875,
          9.09423828125,
          9.11865234375,
          9.14306640625,
          9.16748046875,
          9.19189453125,
          9.21630859375,
          9.24072265625,
          9.26513671875,
          9.28955078125,
          9.31396484375,
          9.33837890625,
          9.36279296875,
          9.38720703125,
          9.41162109375,
          9.43603515625,
          9.46044921875,
          9.48486328125,
          9.50927734375,
          9.53369140625,
          9.55810546875,
          9.58251953125,
          9.60693359375,
          9.63134765625,
          9.65576171875,
          9.68017578125,
          9.70458984375,
          9.72900390625,
          9.75341796875,
          9.77783203125,
          9.80224609375,
          9.82666015625,
          9.85107421875,
          9.87548828125,
          9.89990234375,
          9.92431640625,
          9.94873046875,
          9.97314453125,
          9.99755859375,
          10.02197265625,
          10.04638671875,
          10.07080078125,
          10.09521484375,
          10.11962890625,
          10.14404296875,
          10.16845703125,
          10.19287109375,
          10.21728515625,
          10.24169921875,
          10.26611328125,
          10.29052734375,
          10.31494140625,
          10.33935546875,
          10.36376953125,
          10.38818359375,
          10.41259765625,
          10.43701171875,
          10.46142578125,
          10.48583984375,
          10.51025390625,
          10.53466796875,
          10.55908203125,
          10.58349609375,
          10.60791015625,
          10.63232421875,
          10.65673828125,
          10.68115234375,
          10.70556640625,
          10.72998046875,
          10.75439453125,
          10.77880859375,
          10.80322265625,
          10.82763671875,
          10.85205078125,
          10.87646484375,
          10.90087890625,
          10.92529296875,
          10.94970703125,
          10.97412109375,
          10.99853515625,
          11.02294921875,
          11.04736328125,
          11.07177734375,
          11.09619140625,
          11.12060546875,
          11.14501953125,
          11.16943359375,
          11.19384765625,
          11.21826171875,
          11.24267578125,
          11.26708984375,
          11.29150390625,
          11.31591796875,
          11.34033203125,
          11.36474609375,
          11.38916015625,
          11.41357421875,
          11.43798828125,
          11.46240234375,
          11.48681640625,
          11.51123046875,
          11.53564453125,
          11.56005859375,
          11.58447265625,
          11.60888671875,
          11.63330078125,
          11.65771484375,
          11.68212890625,
          11.70654296875,
          11.73095703125,
          11.75537109375,
          11.77978515625,
          11.80419921875,
          11.82861328125,
          11.85302734375,
          11.87744140625,
          11.90185546875,
          11.92626953125,
          11.95068359375,
          11.97509765625,
          11.99951171875,
          12.02392578125,
          12.04833984375,
          12.07275390625,
          12.09716796875,
          12.12158203125,
          12.14599609375,
          12.17041015625,
          12.19482421875,
          12.21923828125,
          12.24365234375,
          12.26806640625,
          12.29248046875,
          12.31689453125,
          12.34130859375,
          12.36572265625,
          12.39013671875,
          12.41455078125,
          12.43896484375,
          12.46337890625,
          12.48779296875,
          12.51220703125,
          12.53662109375,
          12.56103515625,
          12.58544921875,
          12.60986328125,
          12.63427734375,
          12.65869140625,
          12.68310546875,
          12.70751953125,
          12.73193359375,
          12.75634765625,
          12.78076171875,
          12.80517578125,
          12.82958984375,
          12.85400390625,
          12.87841796875,
          12.90283203125,
          12.92724609375,
          12.95166015625,
          12.97607421875,
          13.00048828125,
          13.02490234375,
          13.04931640625,
          13.07373046875,
          13.09814453125,
          13.12255859375,
          13.14697265625,
          13.17138671875,
          13.19580078125,
          13.22021484375,
          13.24462890625,
          13.26904296875,
          13.29345703125,
          13.31787109375,
          13.34228515625,
          13.36669921875,
          13.39111328125,
          13.41552734375,
          13.43994140625,
          13.46435546875,
          13.48876953125,
          13.51318359375,
          13.53759765625,
          13.56201171875,
          13.58642578125,
          13.61083984375,
          13.63525390625,
          13.65966796875,
          13.68408203125,
          13.70849609375,
          13.73291015625,
          13.75732421875,
          13.78173828125,
          13.80615234375,
          13.83056640625,
          13.85498046875,
          13.87939453125,
          13.90380859375,
          13.92822265625,
          13.95263671875,
          13.97705078125,
          14.00146484375,
          14.02587890625,
          14.05029296875,
          14.07470703125,
          14.09912109375,
          14.12353515625,
          14.14794921875,
          14.17236328125,
          14.19677734375,
          14.22119140625,
          14.24560546875,
          14.27001953125,
          14.29443359375,
          14.31884765625,
          14.34326171875,
          14.36767578125,
          14.39208984375,
          14.41650390625,
          14.44091796875,
          14.46533203125,
          14.48974609375,
          14.51416015625,
          14.53857421875,
          14.56298828125,
          14.58740234375,
          14.61181640625,
          14.63623046875,
          14.66064453125,
          14.68505859375,
          14.70947265625,
          14.73388671875,
          14.75830078125,
          14.78271484375,
          14.80712890625,
          14.83154296875,
          14.85595703125,
          14.88037109375,
          14.90478515625,
          14.92919921875,
          14.95361328125,
          14.97802734375,
          15.00244140625,
          15.02685546875,
          15.05126953125,
          15.07568359375,
          15.10009765625,
          15.12451171875,
          15.14892578125,
          15.17333984375,
          15.19775390625,
          15.22216796875,
          15.24658203125,
          15.27099609375,
          15.29541015625,
          15.31982421875,
          15.34423828125,
          15.36865234375,
          15.39306640625,
          15.41748046875,
          15.44189453125,
          15.46630859375,
          15.49072265625,
          15.51513671875,
          15.53955078125,
          15.56396484375,
          15.58837890625,
          15.61279296875,
          15.63720703125,
          15.66162109375,
          15.68603515625,
          15.71044921875,
          15.73486328125,
          15.75927734375,
          15.78369140625,
          15.80810546875,
          15.83251953125,
          15.85693359375,
          15.88134765625,
          15.90576171875,
          15.93017578125,
          15.95458984375,
          15.97900390625,
          16.00341796875,
          16.02783203125,
          16.05224609375,
          16.07666015625,
          16.10107421875,
          16.12548828125,
          16.14990234375,
          16.17431640625,
          16.19873046875,
          16.22314453125,
          16.24755859375,
          16.27197265625,
          16.29638671875,
          16.32080078125,
          16.34521484375,
          16.36962890625,
          16.39404296875,
          16.41845703125,
          16.44287109375,
          16.46728515625,
          16.49169921875,
          16.51611328125,
          16.54052734375,
          16.56494140625,
          16.58935546875,
          16.61376953125,
          16.63818359375,
          16.66259765625,
          16.68701171875,
          16.71142578125,
          16.73583984375,
          16.76025390625,
          16.78466796875,
          16.80908203125,
          16.83349609375,
          16.85791015625,
          16.88232421875,
          16.90673828125,
          16.93115234375,
          16.95556640625,
          16.97998046875,
          17.00439453125,
          17.02880859375,
          17.05322265625,
          17.07763671875,
          17.10205078125,
          17.12646484375,
          17.15087890625,
          17.17529296875,
          17.19970703125,
          17.22412109375,
          17.24853515625,
          17.27294921875,
          17.29736328125,
          17.32177734375,
          17.34619140625,
          17.37060546875,
          17.39501953125,
          17.41943359375,
          17.44384765625,
          17.46826171875,
          17.49267578125,
          17.51708984375,
          17.54150390625,
          17.56591796875,
          17.59033203125,
          17.61474609375,
          17.63916015625,
          17.66357421875,
          17.68798828125,
          17.71240234375,
          17.73681640625,
          17.76123046875,
          17.78564453125,
          17.81005859375,
          17.83447265625,
          17.85888671875,
          17.88330078125,
          17.90771484375,
          17.93212890625,
          17.95654296875,
          17.98095703125,
          18.00537109375,
          18.02978515625,
          18.05419921875,
          18.07861328125,
          18.10302734375,
          18.12744140625,
          18.15185546875,
          18.17626953125,
          18.20068359375,
          18.22509765625,
          18.24951171875,
          18.27392578125,
          18.29833984375,
          18.32275390625,
          18.34716796875,
          18.37158203125,
          18.39599609375,
          18.42041015625,
          18.44482421875,
          18.46923828125,
          18.49365234375,
          18.51806640625,
          18.54248046875,
          18.56689453125,
          18.59130859375,
          18.61572265625,
          18.64013671875,
          18.66455078125,
          18.68896484375,
          18.71337890625,
          18.73779296875,
          18.76220703125,
          18.78662109375,
          18.81103515625,
          18.83544921875,
          18.85986328125,
          18.88427734375,
          18.90869140625,
          18.93310546875,
          18.95751953125,
          18.98193359375,
          19.00634765625,
          19.03076171875,
          19.05517578125,
          19.07958984375,
          19.10400390625,
          19.12841796875,
          19.15283203125,
          19.17724609375,
          19.20166015625,
          19.22607421875,
          19.25048828125,
          19.27490234375,
          19.29931640625,
          19.32373046875,
          19.34814453125,
          19.37255859375,
          19.39697265625,
          19.42138671875,
          19.44580078125,
          19.47021484375,
          19.49462890625,
          19.51904296875,
          19.54345703125,
          19.56787109375,
          19.59228515625,
          19.61669921875,
          19.64111328125,
          19.66552734375,
          19.68994140625,
          19.71435546875,
          19.73876953125,
          19.76318359375,
          19.78759765625,
          19.81201171875,
          19.83642578125,
          19.86083984375,
          19.88525390625,
          19.90966796875,
          19.93408203125,
          19.95849609375,
          19.98291015625,
          20.00732421875,
          20.03173828125,
          20.05615234375,
          20.08056640625,
          20.10498046875,
          20.12939453125,
          20.15380859375,
          20.17822265625,
          20.20263671875,
          20.22705078125,
          20.25146484375,
          20.27587890625,
          20.30029296875,
          20.32470703125,
          20.34912109375,
          20.37353515625,
          20.39794921875,
          20.42236328125,
          20.44677734375,
          20.47119140625,
          20.49560546875,
          20.52001953125,
          20.54443359375,
          20.56884765625,
          20.59326171875,
          20.61767578125,
          20.64208984375,
          20.66650390625,
          20.69091796875,
          20.71533203125,
          20.73974609375,
          20.76416015625,
          20.78857421875,
          20.81298828125,
          20.83740234375,
          20.86181640625,
          20.88623046875,
          20.91064453125,
          20.93505859375,
          20.95947265625,
          20.98388671875,
          21.00830078125,
          21.03271484375,
          21.05712890625,
          21.08154296875,
          21.10595703125,
          21.13037109375,
          21.15478515625,
          21.17919921875,
          21.20361328125,
          21.22802734375,
          21.25244140625,
          21.27685546875,
          21.30126953125,
          21.32568359375,
          21.35009765625,
          21.37451171875,
          21.39892578125,
          21.42333984375,
          21.44775390625,
          21.47216796875,
          21.49658203125,
          21.52099609375,
          21.54541015625,
          21.56982421875,
          21.59423828125,
          21.61865234375,
          21.64306640625,
          21.66748046875,
          21.69189453125,
          21.71630859375,
          21.74072265625,
          21.76513671875,
          21.78955078125,
          21.81396484375,
          21.83837890625,
          21.86279296875,
          21.88720703125,
          21.91162109375,
          21.93603515625,
          21.96044921875,
          21.98486328125,
          22.00927734375,
          22.03369140625,
          22.05810546875,
          22.08251953125,
          22.10693359375,
          22.13134765625,
          22.15576171875,
          22.18017578125,
          22.20458984375,
          22.22900390625,
          22.25341796875,
          22.27783203125,
          22.30224609375,
          22.32666015625,
          22.35107421875,
          22.37548828125,
          22.39990234375,
          22.42431640625,
          22.44873046875,
          22.47314453125,
          22.49755859375,
          22.52197265625,
          22.54638671875,
          22.57080078125,
          22.59521484375,
          22.61962890625,
          22.64404296875,
          22.66845703125,
          22.69287109375,
          22.71728515625,
          22.74169921875,
          22.76611328125,
          22.79052734375,
          22.81494140625,
          22.83935546875,
          22.86376953125,
          22.88818359375,
          22.91259765625,
          22.93701171875,
          22.96142578125,
          22.98583984375,
          23.01025390625,
          23.03466796875,
          23.05908203125,
          23.08349609375,
          23.10791015625,
          23.13232421875,
          23.15673828125,
          23.18115234375,
          23.20556640625,
          23.22998046875,
          23.25439453125,
          23.27880859375,
          23.30322265625,
          23.32763671875,
          23.35205078125,
          23.37646484375,
          23.40087890625,
          23.42529296875,
          23.44970703125,
          23.47412109375,
          23.49853515625,
          23.52294921875,
          23.54736328125,
          23.57177734375,
          23.59619140625,
          23.62060546875,
          23.64501953125,
          23.66943359375,
          23.69384765625,
          23.71826171875,
          23.74267578125,
          23.76708984375,
          23.79150390625,
          23.81591796875,
          23.84033203125,
          23.86474609375,
          23.88916015625,
          23.91357421875,
          23.93798828125,
          23.96240234375,
          23.98681640625,
          24.01123046875,
          24.03564453125,
          24.06005859375,
          24.08447265625,
          24.10888671875,
          24.13330078125,
          24.15771484375,
          24.18212890625,
          24.20654296875,
          24.23095703125,
          24.25537109375,
          24.27978515625,
          24.30419921875,
          24.32861328125,
          24.35302734375,
          24.37744140625,
          24.40185546875,
          24.42626953125,
          24.45068359375,
          24.47509765625,
          24.49951171875,
          24.52392578125,
          24.54833984375,
          24.57275390625,
          24.59716796875,
          24.62158203125,
          24.64599609375,
          24.67041015625,
          24.69482421875,
          24.71923828125,
          24.74365234375,
          24.76806640625,
          24.79248046875,
          24.81689453125,
          24.84130859375,
          24.86572265625,
          24.89013671875,
          24.91455078125,
          24.93896484375,
          24.96337890625,
          24.98779296875
         ],
         "xaxis": "x",
         "y": [
          0.302422152515168,
          0.21699803672232534,
          0.7219637704174819,
          0.9990875296477589,
          0.31630734070208993,
          0.24692360621004228,
          0.415346030024682,
          0.5445592524433952,
          0.25333591307880554,
          0.6406397390202297,
          0.5745865961487192,
          0.5301729581088822,
          0.520328647352111,
          0.33144486601303497,
          0.5557431202583418,
          0.4498012587864399,
          0.811372957839219,
          0.6418314872810169,
          0.1691894964885834,
          0.2476024173273096,
          0.7167159918690058,
          0.6449093367132638,
          1.132840898979788,
          0.9414445901833408,
          0.6823854346614635,
          0.6101458179989934,
          0.724137626077336,
          0.689170436409313,
          0.67990779880703,
          0.3179327537893313,
          0.2772919835868595,
          0.24703543262685423,
          0.31793346471617273,
          0.17137022062108487,
          0.2736702910755813,
          1.0478266966716683,
          1.3231878230019758,
          0.8867554069079935,
          0.848352278770812,
          0.44790280004999916,
          0.7013379217123734,
          1.1489705135163049,
          0.9840500672736296,
          0.6156372586574373,
          0.257407174639366,
          0.5703298932240104,
          0.7718997133944936,
          0.549136824625025,
          0.5038599975789712,
          0.4404961141169744,
          0.23431840167862447,
          0.5744356180792607,
          0.5048699392070064,
          0.3909141413129764,
          0.6354248061778767,
          0.6505757369179903,
          0.6158538156115905,
          0.662772591439916,
          0.8784570163404483,
          0.41250499796339046,
          0.06434883160886007,
          0.3311760071384673,
          0.7465008989967488,
          0.5446771267507444,
          1.183927354773995,
          1.1260257163645928,
          0.4337167969315927,
          0.308166310624038,
          0.4636154220420833,
          0.5354514818919749,
          0.1392751151309438,
          0.6358656603430194,
          0.43189339231468354,
          0.714207067386273,
          0.504331494494552,
          0.21730772621113967,
          0.3790790297654263,
          0.8328342624916324,
          0.5618696395434674,
          0.3024977080406821,
          0.8870725999120304,
          0.4667100712947485,
          0.6442183244916595,
          0.5746333656948053,
          1.211733629537858,
          0.4940717453059738,
          0.6340356203723905,
          0.7879687060154597,
          0.8762353781581984,
          0.8477717565272004,
          0.2749317587153757,
          0.4405798476248409,
          0.8223902687751108,
          0.652565792831745,
          0.38262393094459063,
          1.0005895763773456,
          0.8093679503507738,
          1.050380837114825,
          0.5981407223223802,
          1.3653685693637696,
          1.053284718114268,
          0.6372182823674056,
          0.723586771227143,
          0.44120566522475024,
          0.33520812741200456,
          0.4278744523546698,
          0.3387076531089568,
          0.8981335820002079,
          0.7051134372854274,
          0.24302707946646274,
          0.18327813231352275,
          0.8624734592111882,
          1.1686270348168413,
          0.5568799510523975,
          0.7363449663922219,
          0.404568661637468,
          0.5726297285997524,
          0.2643464862189392,
          0.2805754723934753,
          0.356632155365582,
          0.4709254912470209,
          0.6240415101547103,
          0.591723842222674,
          0.6616545897567899,
          0.5778105448411303,
          0.38621090730981,
          0.5842538360702327,
          0.2068722481229402,
          0.5782753546281141,
          0.32779478782447946,
          0.6303991451653899,
          0.33237716330502853,
          0.23250602245923332,
          0.48440882097307225,
          0.5888857644257857,
          0.38677971966765795,
          0.6997499022670643,
          0.33363812094862766,
          0.15367928716731344,
          0.10975375339636116,
          0.24121905382940878,
          0.1911806552274647,
          0.5706076441477475,
          0.3675114667713321,
          0.5816504638243456,
          0.5172639046149154,
          0.30414566041647007,
          0.43790911192752996,
          0.8258905943942992,
          0.7886971586135185,
          0.908848249439122,
          0.43980240220142364,
          0.8250287478065147,
          0.570493803624625,
          0.12900201118083382,
          0.39401025645531634,
          0.37857423094391934,
          0.513199410540894,
          0.8347476636155617,
          0.6004155779568062,
          0.19766080802304764,
          0.3309199326706567,
          0.40113428858994726,
          0.5454838743414365,
          0.9092121502728623,
          0.4901403799701565,
          0.6591512954647728,
          1.4590462074843045,
          0.7945203346493531,
          0.6722293840331671,
          0.7831091260049905,
          0.732033957124738,
          0.4038539740682744,
          0.5070793770252522,
          1.0097920953138486,
          0.5804601372352853,
          0.9152222195445403,
          0.8390973626763188,
          0.7666331993591734,
          0.6087236092843036,
          0.35894167305396923,
          1.06992882994093,
          1.241365195564696,
          0.537582692002681,
          0.24283601161443338,
          0.7152626948388316,
          0.5795599950931216,
          0.7624930188559726,
          0.36172504041163256,
          0.366509823789264,
          0.43576475945955107,
          0.16447330878858127,
          0.36016844497334155,
          0.33748327447990517,
          0.4751131344875793,
          0.21191992993760617,
          0.479349567740211,
          0.7722590690676043,
          0.4465242228050197,
          0.23946193091257761,
          0.20426927268319744,
          0.44616103921318406,
          0.8629002958674692,
          0.8418797716081046,
          0.2659288556706223,
          0.3729349243302364,
          0.5426085380686338,
          0.36444644484926125,
          0.8763312346144505,
          0.4825119383544647,
          0.5463572221293186,
          0.632188260076944,
          0.4585091515248856,
          0.3093556488070176,
          0.2457476381539835,
          0.4176394811501872,
          0.5232274122975418,
          0.2700079069663144,
          0.4999332615851727,
          0.9121857374373189,
          0.351650153884644,
          0.9906105350839333,
          0.9109170526469997,
          0.6252017648835766,
          1.2856367209559212,
          0.7731432046828428,
          0.5782619690366261,
          0.7462108662615932,
          0.9814514315612373,
          0.7389881085314871,
          0.4224625074000902,
          0.29600034951491583,
          0.3331522659641039,
          0.5884611541335687,
          0.28153494649913713,
          1.0054443402076296,
          0.4958455064836707,
          0.3468479803243584,
          0.4996562068195044,
          0.41701983917766805,
          0.8031159323095622,
          0.5790104152246324,
          0.3842168063019067,
          0.7275020359869473,
          0.6308749141689405,
          0.28762133546282287,
          0.7879093737987368,
          0.8392022365130494,
          0.8050347848423527,
          0.8902241395952493,
          0.6435887745138243,
          0.7570808781166982,
          0.4199264644600602,
          1.0267588148863174,
          0.711984381524934,
          0.4093484328571332,
          0.3310786079560915,
          0.5358992215718379,
          0.28417946991759857,
          0.4295202568405274,
          0.8839520717993306,
          0.7580735618703028,
          0.4261464582525594,
          0.6781802317958556,
          0.8869574318128253,
          0.5355853517510055,
          0.6034982808416158,
          0.7241187031792067,
          0.7758858521215586,
          0.9962242323005783,
          0.7667521122682743,
          0.3967626967008494,
          0.45297454911703955,
          0.27515031439921744,
          0.47450705355308764,
          0.7187217743990437,
          0.9275963044151427,
          0.5867723109479679,
          0.3983833339456746,
          0.4807795363545834,
          0.3445892859685723,
          0.17079731526339625,
          0.7677363363175049,
          0.7246409091762067,
          0.6018564293427202,
          0.2818957766510634,
          0.5124397609790214,
          0.19940338877133998,
          0.037836074022119096,
          0.0560707543974345,
          0.27545355676059985,
          0.5241005643277724,
          0.568173925953131,
          0.580837800880133,
          0.3669176411648952,
          0.222657724097286,
          0.7209199269062034,
          0.9583713208617981,
          0.4670735261691227,
          0.9175596550448581,
          0.7221544173686871,
          1.1311368414890663,
          1.0969373076334326,
          0.4205522183310495,
          0.45076225998571406,
          0.6943729298844293,
          0.4641988793229523,
          0.255181524383133,
          0.42954976462115096,
          0.26572648948064337,
          0.3994892427636527,
          0.2707007003087527,
          0.7757170076318338,
          0.8268891810443346,
          0.8114981901908271,
          0.8103419076399678,
          0.6823124436162242,
          0.659563981797345,
          0.49210658797693463,
          0.5187364090945138,
          0.6588735347240454,
          0.5706354797182375,
          0.48621778697209594,
          0.5964050438796603,
          0.45572656558997565,
          0.1834034579153372,
          0.4065532347911459,
          0.17317335562666059,
          0.6513370789228996,
          1.3607164454807426,
          0.6652158288386381,
          0.49012850265036734,
          0.8008761460804498,
          0.38898596553483755,
          0.5789757404730742,
          1.0830246875265133,
          1.2291833217131232,
          0.44078278010180616,
          0.7302876664343134,
          0.882647987667571,
          0.3749764656462962,
          0.6785281094579486,
          0.5154253905763633,
          0.8519731943146334,
          0.5357816873463432,
          0.3586755843029722,
          0.4530154311582371,
          0.31415703641789333,
          0.3959671224506828,
          0.6805144397917952,
          0.7211159745272784,
          0.6387619238092324,
          0.8656399098678791,
          0.7170092621587805,
          0.3860548032185095,
          0.3015849746244378,
          0.7886953353789592,
          1.0108428950149384,
          0.30371455204101855,
          0.26105241914070026,
          0.257367828605642,
          0.29526081145632876,
          0.8963604344812359,
          0.4488310201688085,
          1.0059150212632166,
          1.0648028147723418,
          0.24185474600658363,
          0.3681327832924751,
          0.12529448254366415,
          0.07641765790472722,
          0.2367465388969558,
          0.5078760606540882,
          0.8652863159644857,
          0.420547244437117,
          0.8347286386207549,
          1.1643789830578997,
          0.6946726487349235,
          0.2027185048454016,
          0.39041575419768537,
          0.35901428985558853,
          0.6062406940731491,
          0.28905081173552993,
          0.729329478570539,
          0.7015499354079354,
          1.035582376852121,
          0.43067327639190434,
          0.7579685227811546,
          0.8294516296440281,
          0.19241473838680126,
          0.3156743867406361,
          0.3036236594302879,
          0.3240914722158152,
          0.48700384209497694,
          0.3805585278870191,
          0.9007367994085254,
          0.480233839532353,
          0.378711321472325,
          0.2421322075148839,
          0.21171331699721083,
          0.2131984294142621,
          0.7015426972733995,
          0.5374313888669375,
          0.3885754659225525,
          0.532419002203322,
          1.0945461330380315,
          0.918946858909461,
          0.43054462917231584,
          0.7618254894768886,
          1.0466869940775472,
          0.5525088755875605,
          0.2637591628923324,
          0.46568592691807575,
          0.732485610914913,
          0.3322658788766909,
          0.6334845441644932,
          0.6340350206363505,
          0.2906706284063311,
          0.9325394642340021,
          1.1075142376320404,
          0.2998341738752099,
          0.17352864681334634,
          0.3484608818518683,
          0.4366787505083577,
          0.6674159952480008,
          0.5912411606164285,
          1.0531698345028269,
          1.4266575435148416,
          0.8462414229478906,
          1.0595548571207134,
          0.9463893371849547,
          1.311667023472872,
          1.0987213924784385,
          0.793309506014032,
          0.8862335622679482,
          0.9208513704165242,
          0.9151285921854311,
          0.6479903118064103,
          0.9158959289345785,
          0.665078737069697,
          0.42387898171356164,
          0.3669175258834334,
          0.4975309981428552,
          0.5872931251395241,
          0.26348938957163504,
          0.4027318162723005,
          0.9303104422658866,
          0.953137861244832,
          0.6403422223857168,
          0.27778212614283,
          1.15819041142738,
          0.9730391645462548,
          0.729427181704072,
          0.4227925137048789,
          0.31156232647834814,
          1.0884392965073526,
          0.657099928969492,
          0.08626016373636133,
          0.4013642790766402,
          0.6903964896282196,
          0.5034581666495024,
          0.3413152216506457,
          0.2904782109120158,
          1.0720251370709464,
          1.3914530789987478,
          0.7916016578767215,
          0.39353027285084297,
          0.4722870403701439,
          0.7398076125763609,
          1.0104147402648482,
          0.8924171530306155,
          0.5621281987243937,
          0.581939761900166,
          0.336759795418564,
          0.59267428142601,
          0.6359950737210649,
          0.41658727336136137,
          0.5932602620019156,
          0.843250798569406,
          1.169024840955324,
          0.9184822331092125,
          0.42819474318046885,
          0.83143272446076,
          0.8290027992511015,
          0.5880337189749529,
          1.1329861808679766,
          0.40034214363586546,
          0.9406858864281369,
          0.7437617822248627,
          0.3155691803376796,
          0.4432008940696584,
          0.916200817902215,
          0.7624172492681729,
          1.1721264360728145,
          0.7050847374516056,
          0.704495093807648,
          1.170452426884655,
          0.4576106590452277,
          0.2754521625871358,
          0.4898121262756743,
          0.8927235058025922,
          0.837488378698207,
          1.4209747898592442,
          0.4495494928025957,
          0.5721011385043603,
          1.3145364272086701,
          1.2398958377203224,
          1.0712376376921446,
          1.2110319319581446,
          0.49156476349683,
          1.0078015417585096,
          0.9928687344565958,
          0.7632630441276409,
          0.9903633573042834,
          0.6560782252891744,
          1.083370427403123,
          0.8761218641248707,
          0.9692603013003959,
          0.953406928755197,
          0.4082932056074709,
          0.8364771570348005,
          1.1349577808330298,
          0.5593879888866229,
          0.5330534288658909,
          0.585185857349958,
          0.4254015892642143,
          1.066437995896388,
          1.0826687554563146,
          0.511935259545132,
          0.45002373955977215,
          0.5652646270902224,
          0.48511968460715904,
          0.7388693777424697,
          0.5173261944380274,
          0.6848522336268685,
          0.751687875202851,
          0.3405359537214131,
          0.5826064764028831,
          0.7987108464799407,
          0.8856434569536391,
          0.5284698318925163,
          0.5648084442191361,
          0.45558570193427145,
          0.41950562279988096,
          0.617893001044058,
          0.7354894578553639,
          0.5622237594342729,
          1.0734585983330658,
          1.0934905050930452,
          0.5565872277653401,
          0.8903019042134386,
          0.6209318627710787,
          0.22516199615003119,
          0.5376483339055596,
          0.28144218665852244,
          0.37350548651141324,
          0.6474602385567714,
          0.6992677734418337,
          0.29426627511669606,
          0.5256399103011311,
          0.37792455087866794,
          0.5250104951527248,
          0.7912879299394395,
          0.6009595948312473,
          1.365255400728211,
          0.9454098714487259,
          0.7669034996416457,
          0.5327894569974606,
          0.411868510728293,
          0.7731325543876882,
          0.35691825445179914,
          0.9086741669359291,
          0.6031082804842015,
          0.5606355896783707,
          0.23013498162299478,
          0.7722291871617786,
          1.1397103533463566,
          0.8883137060509039,
          0.7600454641180716,
          0.9976889680385013,
          1.1646691364521475,
          0.6962197790178606,
          0.5768841246959469,
          0.8468833951689194,
          1.113378184110922,
          0.26928041325547125,
          0.3880522794919941,
          0.3851513888147914,
          0.7039953384639759,
          0.7810942749410746,
          0.8113092066633021,
          0.42888349707015794,
          0.6542012138914226,
          1.0003428630022557,
          0.8773256771351601,
          0.43126227194986055,
          0.7724303319401282,
          0.6851725239618958,
          0.2397462413564819,
          0.6736126957529922,
          1.130139377716179,
          1.0553746791068046,
          0.6956923420642614,
          0.9430142718701913,
          1.0499395205997823,
          0.7838701292499963,
          0.43397362433770137,
          1.2858286055265349,
          0.9428785083823379,
          1.304040920886359,
          0.9294980679489999,
          1.187966204215801,
          0.4773910491643093,
          0.43178847529898967,
          0.6489323337118721,
          0.1916871309556905,
          1.1705361432272006,
          0.600384020568884,
          1.2248664916393104,
          0.8304220526735308,
          0.370267309222382,
          0.21717783292551254,
          0.22543803031278964,
          0.4487354460549244,
          0.3465339218038938,
          0.5070250553028306,
          0.5261544522486596,
          0.9438273015623533,
          0.6320808948353891,
          0.425342578110242,
          0.670534753914592,
          0.8517603274926616,
          0.8437346648763846,
          1.11254906536688,
          0.5031622491198092,
          0.9528413879846342,
          1.0230137161864208,
          0.22539073992415015,
          0.28406446689647125,
          0.4269303749534211,
          0.7735235167751031,
          1.5382926453896084,
          1.2514838220381757,
          0.638884860610079,
          1.130444200108038,
          0.3133560210072996,
          0.485033166571031,
          0.9493587078358745,
          1.0384532730884253,
          0.7570250692123944,
          0.9913116821424515,
          0.6026951137928931,
          0.4055109988832133,
          0.22258043195987404,
          0.4692181572796832,
          0.5582623912147402,
          0.467896273355322,
          0.33383209277189685,
          0.7332169803905217,
          0.35263166695371684,
          0.9484431481666735,
          0.658246231590017,
          0.07894976216739595,
          0.38590202075975866,
          0.6628933897545819,
          0.8178036743880917,
          0.6427380057679015,
          0.39389912530346427,
          0.7345924198921578,
          0.9424909696155761,
          0.8738450806335141,
          1.7817019115608024,
          0.8377852445210681,
          0.5304701110026397,
          0.5240554488598047,
          0.7881563417854821,
          0.6302117233338077,
          0.3966086653603175,
          0.778960039894014,
          0.5352301918899639,
          0.19153204874935406,
          0.21279784152814735,
          0.12751440770576866,
          0.14401804802437185,
          0.4552026936923488,
          0.5536402908025939,
          0.2422349522633743,
          0.6945615670429567,
          0.9609191393444719,
          0.624336183418205,
          0.6295834099635842,
          0.47132906947900166,
          0.3518391907045818,
          0.5751616464831297,
          0.6345202618613759,
          0.45877593688553914,
          0.6229960688916321,
          0.9718356190411135,
          0.6083529261624504,
          1.4923109969995811,
          0.6041860381622786,
          0.47808819307740963,
          0.3099150466446543,
          0.6517573660847622,
          0.6970336406826733,
          0.7945578141648065,
          0.38204959618389034,
          0.324519914615777,
          0.7717589725900359,
          0.39675100491073634,
          0.5404303611972747,
          0.5648558829028796,
          0.6421300326473449,
          0.24929878651173937,
          0.724517535628697,
          0.8958744379892669,
          0.28304122321251785,
          0.44309972564317734,
          0.43314403745163343,
          0.24820066884079928,
          1.0804042593782515,
          1.0625650525540575,
          0.37686433090022686,
          1.0719812609301802,
          0.8099017600895525,
          0.645461849099431,
          1.1008808329778246,
          0.4073396143408648,
          1.0971082859847918,
          1.022349478048314,
          0.4691198325318973,
          0.4576903996905228,
          0.33174141958753206,
          0.1504351030719168,
          0.3123892870571399,
          0.21266200601702007,
          0.5832723459825964,
          0.4376502351825178,
          0.4448023324490086,
          0.32606874764420707,
          0.6611783494675516,
          0.4563761668781069,
          1.003994444576741,
          0.43588767324755234,
          0.5255627843491836,
          0.6255627018219542,
          0.31472844935292443,
          0.37760749857074827,
          0.4943034517662241,
          0.7642857765844426,
          1.06193039336535,
          1.0479523426702868,
          0.7522218858990649,
          0.9061105635256284,
          0.6140325528390608,
          0.6593048945388471,
          0.6835801920274973,
          0.43500639298765875,
          0.22789092614936168,
          0.4630662049169044,
          0.8771257158197399,
          0.9430477292125744,
          0.3835421437316726,
          0.9619947486572147,
          0.805667943070841,
          0.2849843816218234,
          0.7681272227178515,
          1.1452491073115731,
          1.029988040167433,
          0.7149197942093634,
          0.5725741304624273,
          0.6727740927448607,
          0.4635179729709843,
          0.9267437371712263,
          0.9095994630070932,
          0.33758180260993687,
          0.7081378583075957,
          0.4813783986294546,
          0.779501445790143,
          0.5334043951687676,
          0.2998504728462198,
          0.293322468126302,
          0.6460225438546421,
          0.5732207328709555,
          1.0598326046562663,
          0.8848990796536154,
          0.6996342093457597,
          0.25700587455215,
          0.45875841286804153,
          0.18704621590972953,
          0.4717486951520549,
          0.7247714179255906,
          0.5891507973223178,
          0.3508624688510251,
          0.8760267361393881,
          0.6383185989296676,
          1.0508788861589051,
          1.0929324128642988,
          0.512902989227439,
          0.5886007279695958,
          0.9384133349314965,
          0.9575785152818718,
          0.4907790485748554,
          0.8774586384520205,
          0.20926472185190254,
          0.9992540651803445,
          0.5751343045066621,
          1.1115918434796144,
          0.41355969303817197,
          0.16214396775764545,
          0.8248324853701271,
          0.4561343095977349,
          0.7079417334228919,
          0.5996238102348407,
          0.7227644233964574,
          0.5540151870951953,
          0.41098564679695304,
          0.5201772790123304,
          0.5159289212588926,
          0.2916805903850163,
          0.522100814198696,
          0.8257535344624656,
          0.6966493078968956,
          0.33678564368718034,
          1.102969924674104,
          0.7312045281228292,
          0.48661014454101514,
          0.3199826060860993,
          0.3617873128694558,
          0.7919490896291779,
          0.31073238074957854,
          0.5597977436351456,
          0.37515238373362936,
          0.49998814725047225,
          0.6682249759471882,
          0.19284704779034947,
          0.3076279020357773,
          0.6767610478910697,
          0.6749788691080999,
          1.2527768255289602,
          1.2945245416193913,
          0.9151057156120741,
          0.8947152994504575,
          0.27833994214823377,
          0.30698447117095473,
          0.5029678235456855,
          0.8415389804042384,
          0.5482304517743783,
          0.9029512302836791,
          0.47146291371197524,
          0.19464837220732167,
          0.08175660455895012,
          0.21905748397367236,
          0.8624980020468882,
          0.8573975448997607,
          0.48558879043892494,
          1.1633449801057605,
          0.7012205668755288,
          0.15948174928980324,
          0.519970431626241,
          0.8753525312906909,
          0.481046093078096,
          0.9910150993067144,
          0.974605782687259,
          0.7355124310881618,
          0.3851701422784025,
          0.6862458045005579,
          0.8188389298926424,
          0.48757072592382955,
          0.6767199582628418,
          0.9417566241544804,
          0.6959499589253488,
          0.5209893625901938,
          0.5178310649103407,
          0.11858276632036713,
          0.8483774562395419,
          1.1180858862025964,
          0.41857669556476046,
          0.9856529781220167,
          0.9271703177260283,
          0.53614271406708,
          0.35586851699498867,
          0.3841996533524582,
          0.41837727646998274,
          0.8711159770059209,
          0.6839611458235165,
          0.7117825130616375,
          1.0022269437587028,
          0.4903666995294323,
          0.27141296999994974,
          0.2850920758279436,
          0.2574604909987798,
          0.5015605537525603,
          0.6257050390730201,
          0.6001124880059923,
          0.37065893844455144,
          0.40695985160645537,
          0.5667511140128791,
          0.47001892246876786,
          0.977031405040619,
          0.7630753501903645,
          0.5932652232133856,
          0.5338120177022352,
          0.2000135526201698,
          0.4159862568954986,
          0.5072109333729841,
          0.4947865885801578,
          1.1161217345989822,
          0.49651188561425763,
          0.8357200209152521,
          0.4947593048104374,
          0.6252370411843069,
          0.7676408927200549,
          0.6054511910326972,
          0.33348416664132285,
          0.37257618173635476,
          0.6190890009146043,
          0.6080499872758353,
          0.39281153082062614,
          0.7295910751241694,
          0.6270312434198971,
          0.3357135696548405,
          0.30188535827170904,
          0.17883237128032736,
          0.5307773128833619,
          0.46465597173497664,
          0.7270462473026205,
          0.8142175347122951,
          0.45612257994261457,
          1.0739981785660295,
          0.6709869326226781,
          0.5041812275299666,
          0.28001800981075575,
          0.49636296989071177,
          0.5084511482816331,
          0.4298405767153072,
          0.48654450071968397,
          0.5510984185291828,
          0.5405840509181048,
          0.3793137977635125,
          0.8878780900517458,
          0.8791027796458835,
          0.8126120909012624,
          0.5327169292523237,
          0.20176270858629053,
          0.26049845788186377,
          0.9147512802118584,
          0.8458014634838837,
          0.8473246610525298,
          0.5733619856050696,
          0.39769960224436846,
          0.8984334610686794,
          0.9867006745237636,
          0.8962693620373918,
          1.3336737954152187,
          0.7482772852338017,
          0.14848337987341453,
          0.3805644323504655,
          0.20458499992441959,
          0.6851342440222181,
          1.0852106313462617,
          0.7191784011078728,
          0.8649621519901631,
          0.49988016290642445,
          0.7300403621722424,
          0.5658887805506578,
          0.35350020943741267,
          0.8593602383451608,
          0.7830770787668587,
          0.12871342699322041,
          0.611297951502872,
          0.8531516929420071,
          0.31011802289915025,
          1.1002940253707159,
          1.0991235990471113,
          0.4939991162413512,
          0.43478133991409396,
          0.26726371041425995,
          0.3592565601121924,
          0.3707546367986503,
          0.21303832886485308,
          0.3317115676976468,
          0.6347326941443806,
          0.8019802694016427,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0
         ],
         "yaxis": "y"
        }
       ],
       "layout": {
        "legend": {
         "tracegroupgap": 0
        },
        "margin": {
         "t": 60
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "x"
         }
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "y"
         }
        }
       }
      },
      "text/html": [
       "<div>                            <div id=\"60afbcec-c36c-4fa2-885d-59dceed1567a\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                require([\"plotly\"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"60afbcec-c36c-4fa2-885d-59dceed1567a\")) {                    Plotly.newPlot(                        \"60afbcec-c36c-4fa2-885d-59dceed1567a\",                        [{\"hovertemplate\":\"x=%{x}<br>y=%{y}<extra></extra>\",\"legendgroup\":\"\",\"line\":{\"color\":\"#636efa\",\"dash\":\"solid\"},\"marker\":{\"symbol\":\"circle\"},\"mode\":\"lines\",\"name\":\"\",\"showlegend\":false,\"x\":[0.01220703125,0.03662109375,0.06103515625,0.08544921875,0.10986328125,0.13427734375,0.15869140625,0.18310546875,0.20751953125,0.23193359375,0.25634765625,0.28076171875,0.30517578125,0.32958984375,0.35400390625,0.37841796875,0.40283203125,0.42724609375,0.45166015625,0.47607421875,0.50048828125,0.52490234375,0.54931640625,0.57373046875,0.59814453125,0.62255859375,0.64697265625,0.67138671875,0.69580078125,0.72021484375,0.74462890625,0.76904296875,0.79345703125,0.81787109375,0.84228515625,0.86669921875,0.89111328125,0.91552734375,0.93994140625,0.96435546875,0.98876953125,1.01318359375,1.03759765625,1.06201171875,1.08642578125,1.11083984375,1.13525390625,1.15966796875,1.18408203125,1.20849609375,1.23291015625,1.25732421875,1.28173828125,1.30615234375,1.33056640625,1.35498046875,1.37939453125,1.40380859375,1.42822265625,1.45263671875,1.47705078125,1.50146484375,1.52587890625,1.55029296875,1.57470703125,1.59912109375,1.62353515625,1.64794921875,1.67236328125,1.69677734375,1.72119140625,1.74560546875,1.77001953125,1.79443359375,1.81884765625,1.84326171875,1.86767578125,1.89208984375,1.91650390625,1.94091796875,1.96533203125,1.98974609375,2.01416015625,2.03857421875,2.06298828125,2.08740234375,2.11181640625,2.13623046875,2.16064453125,2.18505859375,2.20947265625,2.23388671875,2.25830078125,2.28271484375,2.30712890625,2.33154296875,2.35595703125,2.38037109375,2.40478515625,2.42919921875,2.45361328125,2.47802734375,2.50244140625,2.52685546875,2.55126953125,2.57568359375,2.60009765625,2.62451171875,2.64892578125,2.67333984375,2.69775390625,2.72216796875,2.74658203125,2.77099609375,2.79541015625,2.81982421875,2.84423828125,2.86865234375,2.89306640625,2.91748046875,2.94189453125,2.96630859375,2.99072265625,3.01513671875,3.03955078125,3.06396484375,3.08837890625,3.11279296875,3.13720703125,3.16162109375,3.18603515625,3.21044921875,3.23486328125,3.25927734375,3.28369140625,3.30810546875,3.33251953125,3.35693359375,3.38134765625,3.40576171875,3.43017578125,3.45458984375,3.47900390625,3.50341796875,3.52783203125,3.55224609375,3.57666015625,3.60107421875,3.62548828125,3.64990234375,3.67431640625,3.69873046875,3.72314453125,3.74755859375,3.77197265625,3.79638671875,3.82080078125,3.84521484375,3.86962890625,3.89404296875,3.91845703125,3.94287109375,3.96728515625,3.99169921875,4.01611328125,4.04052734375,4.06494140625,4.08935546875,4.11376953125,4.13818359375,4.16259765625,4.18701171875,4.21142578125,4.23583984375,4.26025390625,4.28466796875,4.30908203125,4.33349609375,4.35791015625,4.38232421875,4.40673828125,4.43115234375,4.45556640625,4.47998046875,4.50439453125,4.52880859375,4.55322265625,4.57763671875,4.60205078125,4.62646484375,4.65087890625,4.67529296875,4.69970703125,4.72412109375,4.74853515625,4.77294921875,4.79736328125,4.82177734375,4.84619140625,4.87060546875,4.89501953125,4.91943359375,4.94384765625,4.96826171875,4.99267578125,5.01708984375,5.04150390625,5.06591796875,5.09033203125,5.11474609375,5.13916015625,5.16357421875,5.18798828125,5.21240234375,5.23681640625,5.26123046875,5.28564453125,5.31005859375,5.33447265625,5.35888671875,5.38330078125,5.40771484375,5.43212890625,5.45654296875,5.48095703125,5.50537109375,5.52978515625,5.55419921875,5.57861328125,5.60302734375,5.62744140625,5.65185546875,5.67626953125,5.70068359375,5.72509765625,5.74951171875,5.77392578125,5.79833984375,5.82275390625,5.84716796875,5.87158203125,5.89599609375,5.92041015625,5.94482421875,5.96923828125,5.99365234375,6.01806640625,6.04248046875,6.06689453125,6.09130859375,6.11572265625,6.14013671875,6.16455078125,6.18896484375,6.21337890625,6.23779296875,6.26220703125,6.28662109375,6.31103515625,6.33544921875,6.35986328125,6.38427734375,6.40869140625,6.43310546875,6.45751953125,6.48193359375,6.50634765625,6.53076171875,6.55517578125,6.57958984375,6.60400390625,6.62841796875,6.65283203125,6.67724609375,6.70166015625,6.72607421875,6.75048828125,6.77490234375,6.79931640625,6.82373046875,6.84814453125,6.87255859375,6.89697265625,6.92138671875,6.94580078125,6.97021484375,6.99462890625,7.01904296875,7.04345703125,7.06787109375,7.09228515625,7.11669921875,7.14111328125,7.16552734375,7.18994140625,7.21435546875,7.23876953125,7.26318359375,7.28759765625,7.31201171875,7.33642578125,7.36083984375,7.38525390625,7.40966796875,7.43408203125,7.45849609375,7.48291015625,7.50732421875,7.53173828125,7.55615234375,7.58056640625,7.60498046875,7.62939453125,7.65380859375,7.67822265625,7.70263671875,7.72705078125,7.75146484375,7.77587890625,7.80029296875,7.82470703125,7.84912109375,7.87353515625,7.89794921875,7.92236328125,7.94677734375,7.97119140625,7.99560546875,8.02001953125,8.04443359375,8.06884765625,8.09326171875,8.11767578125,8.14208984375,8.16650390625,8.19091796875,8.21533203125,8.23974609375,8.26416015625,8.28857421875,8.31298828125,8.33740234375,8.36181640625,8.38623046875,8.41064453125,8.43505859375,8.45947265625,8.48388671875,8.50830078125,8.53271484375,8.55712890625,8.58154296875,8.60595703125,8.63037109375,8.65478515625,8.67919921875,8.70361328125,8.72802734375,8.75244140625,8.77685546875,8.80126953125,8.82568359375,8.85009765625,8.87451171875,8.89892578125,8.92333984375,8.94775390625,8.97216796875,8.99658203125,9.02099609375,9.04541015625,9.06982421875,9.09423828125,9.11865234375,9.14306640625,9.16748046875,9.19189453125,9.21630859375,9.24072265625,9.26513671875,9.28955078125,9.31396484375,9.33837890625,9.36279296875,9.38720703125,9.41162109375,9.43603515625,9.46044921875,9.48486328125,9.50927734375,9.53369140625,9.55810546875,9.58251953125,9.60693359375,9.63134765625,9.65576171875,9.68017578125,9.70458984375,9.72900390625,9.75341796875,9.77783203125,9.80224609375,9.82666015625,9.85107421875,9.87548828125,9.89990234375,9.92431640625,9.94873046875,9.97314453125,9.99755859375,10.02197265625,10.04638671875,10.07080078125,10.09521484375,10.11962890625,10.14404296875,10.16845703125,10.19287109375,10.21728515625,10.24169921875,10.26611328125,10.29052734375,10.31494140625,10.33935546875,10.36376953125,10.38818359375,10.41259765625,10.43701171875,10.46142578125,10.48583984375,10.51025390625,10.53466796875,10.55908203125,10.58349609375,10.60791015625,10.63232421875,10.65673828125,10.68115234375,10.70556640625,10.72998046875,10.75439453125,10.77880859375,10.80322265625,10.82763671875,10.85205078125,10.87646484375,10.90087890625,10.92529296875,10.94970703125,10.97412109375,10.99853515625,11.02294921875,11.04736328125,11.07177734375,11.09619140625,11.12060546875,11.14501953125,11.16943359375,11.19384765625,11.21826171875,11.24267578125,11.26708984375,11.29150390625,11.31591796875,11.34033203125,11.36474609375,11.38916015625,11.41357421875,11.43798828125,11.46240234375,11.48681640625,11.51123046875,11.53564453125,11.56005859375,11.58447265625,11.60888671875,11.63330078125,11.65771484375,11.68212890625,11.70654296875,11.73095703125,11.75537109375,11.77978515625,11.80419921875,11.82861328125,11.85302734375,11.87744140625,11.90185546875,11.92626953125,11.95068359375,11.97509765625,11.99951171875,12.02392578125,12.04833984375,12.07275390625,12.09716796875,12.12158203125,12.14599609375,12.17041015625,12.19482421875,12.21923828125,12.24365234375,12.26806640625,12.29248046875,12.31689453125,12.34130859375,12.36572265625,12.39013671875,12.41455078125,12.43896484375,12.46337890625,12.48779296875,12.51220703125,12.53662109375,12.56103515625,12.58544921875,12.60986328125,12.63427734375,12.65869140625,12.68310546875,12.70751953125,12.73193359375,12.75634765625,12.78076171875,12.80517578125,12.82958984375,12.85400390625,12.87841796875,12.90283203125,12.92724609375,12.95166015625,12.97607421875,13.00048828125,13.02490234375,13.04931640625,13.07373046875,13.09814453125,13.12255859375,13.14697265625,13.17138671875,13.19580078125,13.22021484375,13.24462890625,13.26904296875,13.29345703125,13.31787109375,13.34228515625,13.36669921875,13.39111328125,13.41552734375,13.43994140625,13.46435546875,13.48876953125,13.51318359375,13.53759765625,13.56201171875,13.58642578125,13.61083984375,13.63525390625,13.65966796875,13.68408203125,13.70849609375,13.73291015625,13.75732421875,13.78173828125,13.80615234375,13.83056640625,13.85498046875,13.87939453125,13.90380859375,13.92822265625,13.95263671875,13.97705078125,14.00146484375,14.02587890625,14.05029296875,14.07470703125,14.09912109375,14.12353515625,14.14794921875,14.17236328125,14.19677734375,14.22119140625,14.24560546875,14.27001953125,14.29443359375,14.31884765625,14.34326171875,14.36767578125,14.39208984375,14.41650390625,14.44091796875,14.46533203125,14.48974609375,14.51416015625,14.53857421875,14.56298828125,14.58740234375,14.61181640625,14.63623046875,14.66064453125,14.68505859375,14.70947265625,14.73388671875,14.75830078125,14.78271484375,14.80712890625,14.83154296875,14.85595703125,14.88037109375,14.90478515625,14.92919921875,14.95361328125,14.97802734375,15.00244140625,15.02685546875,15.05126953125,15.07568359375,15.10009765625,15.12451171875,15.14892578125,15.17333984375,15.19775390625,15.22216796875,15.24658203125,15.27099609375,15.29541015625,15.31982421875,15.34423828125,15.36865234375,15.39306640625,15.41748046875,15.44189453125,15.46630859375,15.49072265625,15.51513671875,15.53955078125,15.56396484375,15.58837890625,15.61279296875,15.63720703125,15.66162109375,15.68603515625,15.71044921875,15.73486328125,15.75927734375,15.78369140625,15.80810546875,15.83251953125,15.85693359375,15.88134765625,15.90576171875,15.93017578125,15.95458984375,15.97900390625,16.00341796875,16.02783203125,16.05224609375,16.07666015625,16.10107421875,16.12548828125,16.14990234375,16.17431640625,16.19873046875,16.22314453125,16.24755859375,16.27197265625,16.29638671875,16.32080078125,16.34521484375,16.36962890625,16.39404296875,16.41845703125,16.44287109375,16.46728515625,16.49169921875,16.51611328125,16.54052734375,16.56494140625,16.58935546875,16.61376953125,16.63818359375,16.66259765625,16.68701171875,16.71142578125,16.73583984375,16.76025390625,16.78466796875,16.80908203125,16.83349609375,16.85791015625,16.88232421875,16.90673828125,16.93115234375,16.95556640625,16.97998046875,17.00439453125,17.02880859375,17.05322265625,17.07763671875,17.10205078125,17.12646484375,17.15087890625,17.17529296875,17.19970703125,17.22412109375,17.24853515625,17.27294921875,17.29736328125,17.32177734375,17.34619140625,17.37060546875,17.39501953125,17.41943359375,17.44384765625,17.46826171875,17.49267578125,17.51708984375,17.54150390625,17.56591796875,17.59033203125,17.61474609375,17.63916015625,17.66357421875,17.68798828125,17.71240234375,17.73681640625,17.76123046875,17.78564453125,17.81005859375,17.83447265625,17.85888671875,17.88330078125,17.90771484375,17.93212890625,17.95654296875,17.98095703125,18.00537109375,18.02978515625,18.05419921875,18.07861328125,18.10302734375,18.12744140625,18.15185546875,18.17626953125,18.20068359375,18.22509765625,18.24951171875,18.27392578125,18.29833984375,18.32275390625,18.34716796875,18.37158203125,18.39599609375,18.42041015625,18.44482421875,18.46923828125,18.49365234375,18.51806640625,18.54248046875,18.56689453125,18.59130859375,18.61572265625,18.64013671875,18.66455078125,18.68896484375,18.71337890625,18.73779296875,18.76220703125,18.78662109375,18.81103515625,18.83544921875,18.85986328125,18.88427734375,18.90869140625,18.93310546875,18.95751953125,18.98193359375,19.00634765625,19.03076171875,19.05517578125,19.07958984375,19.10400390625,19.12841796875,19.15283203125,19.17724609375,19.20166015625,19.22607421875,19.25048828125,19.27490234375,19.29931640625,19.32373046875,19.34814453125,19.37255859375,19.39697265625,19.42138671875,19.44580078125,19.47021484375,19.49462890625,19.51904296875,19.54345703125,19.56787109375,19.59228515625,19.61669921875,19.64111328125,19.66552734375,19.68994140625,19.71435546875,19.73876953125,19.76318359375,19.78759765625,19.81201171875,19.83642578125,19.86083984375,19.88525390625,19.90966796875,19.93408203125,19.95849609375,19.98291015625,20.00732421875,20.03173828125,20.05615234375,20.08056640625,20.10498046875,20.12939453125,20.15380859375,20.17822265625,20.20263671875,20.22705078125,20.25146484375,20.27587890625,20.30029296875,20.32470703125,20.34912109375,20.37353515625,20.39794921875,20.42236328125,20.44677734375,20.47119140625,20.49560546875,20.52001953125,20.54443359375,20.56884765625,20.59326171875,20.61767578125,20.64208984375,20.66650390625,20.69091796875,20.71533203125,20.73974609375,20.76416015625,20.78857421875,20.81298828125,20.83740234375,20.86181640625,20.88623046875,20.91064453125,20.93505859375,20.95947265625,20.98388671875,21.00830078125,21.03271484375,21.05712890625,21.08154296875,21.10595703125,21.13037109375,21.15478515625,21.17919921875,21.20361328125,21.22802734375,21.25244140625,21.27685546875,21.30126953125,21.32568359375,21.35009765625,21.37451171875,21.39892578125,21.42333984375,21.44775390625,21.47216796875,21.49658203125,21.52099609375,21.54541015625,21.56982421875,21.59423828125,21.61865234375,21.64306640625,21.66748046875,21.69189453125,21.71630859375,21.74072265625,21.76513671875,21.78955078125,21.81396484375,21.83837890625,21.86279296875,21.88720703125,21.91162109375,21.93603515625,21.96044921875,21.98486328125,22.00927734375,22.03369140625,22.05810546875,22.08251953125,22.10693359375,22.13134765625,22.15576171875,22.18017578125,22.20458984375,22.22900390625,22.25341796875,22.27783203125,22.30224609375,22.32666015625,22.35107421875,22.37548828125,22.39990234375,22.42431640625,22.44873046875,22.47314453125,22.49755859375,22.52197265625,22.54638671875,22.57080078125,22.59521484375,22.61962890625,22.64404296875,22.66845703125,22.69287109375,22.71728515625,22.74169921875,22.76611328125,22.79052734375,22.81494140625,22.83935546875,22.86376953125,22.88818359375,22.91259765625,22.93701171875,22.96142578125,22.98583984375,23.01025390625,23.03466796875,23.05908203125,23.08349609375,23.10791015625,23.13232421875,23.15673828125,23.18115234375,23.20556640625,23.22998046875,23.25439453125,23.27880859375,23.30322265625,23.32763671875,23.35205078125,23.37646484375,23.40087890625,23.42529296875,23.44970703125,23.47412109375,23.49853515625,23.52294921875,23.54736328125,23.57177734375,23.59619140625,23.62060546875,23.64501953125,23.66943359375,23.69384765625,23.71826171875,23.74267578125,23.76708984375,23.79150390625,23.81591796875,23.84033203125,23.86474609375,23.88916015625,23.91357421875,23.93798828125,23.96240234375,23.98681640625,24.01123046875,24.03564453125,24.06005859375,24.08447265625,24.10888671875,24.13330078125,24.15771484375,24.18212890625,24.20654296875,24.23095703125,24.25537109375,24.27978515625,24.30419921875,24.32861328125,24.35302734375,24.37744140625,24.40185546875,24.42626953125,24.45068359375,24.47509765625,24.49951171875,24.52392578125,24.54833984375,24.57275390625,24.59716796875,24.62158203125,24.64599609375,24.67041015625,24.69482421875,24.71923828125,24.74365234375,24.76806640625,24.79248046875,24.81689453125,24.84130859375,24.86572265625,24.89013671875,24.91455078125,24.93896484375,24.96337890625,24.98779296875],\"xaxis\":\"x\",\"y\":[0.302422152515168,0.21699803672232534,0.7219637704174819,0.9990875296477589,0.31630734070208993,0.24692360621004228,0.415346030024682,0.5445592524433952,0.25333591307880554,0.6406397390202297,0.5745865961487192,0.5301729581088822,0.520328647352111,0.33144486601303497,0.5557431202583418,0.4498012587864399,0.811372957839219,0.6418314872810169,0.1691894964885834,0.2476024173273096,0.7167159918690058,0.6449093367132638,1.132840898979788,0.9414445901833408,0.6823854346614635,0.6101458179989934,0.724137626077336,0.689170436409313,0.67990779880703,0.3179327537893313,0.2772919835868595,0.24703543262685423,0.31793346471617273,0.17137022062108487,0.2736702910755813,1.0478266966716683,1.3231878230019758,0.8867554069079935,0.848352278770812,0.44790280004999916,0.7013379217123734,1.1489705135163049,0.9840500672736296,0.6156372586574373,0.257407174639366,0.5703298932240104,0.7718997133944936,0.549136824625025,0.5038599975789712,0.4404961141169744,0.23431840167862447,0.5744356180792607,0.5048699392070064,0.3909141413129764,0.6354248061778767,0.6505757369179903,0.6158538156115905,0.662772591439916,0.8784570163404483,0.41250499796339046,0.06434883160886007,0.3311760071384673,0.7465008989967488,0.5446771267507444,1.183927354773995,1.1260257163645928,0.4337167969315927,0.308166310624038,0.4636154220420833,0.5354514818919749,0.1392751151309438,0.6358656603430194,0.43189339231468354,0.714207067386273,0.504331494494552,0.21730772621113967,0.3790790297654263,0.8328342624916324,0.5618696395434674,0.3024977080406821,0.8870725999120304,0.4667100712947485,0.6442183244916595,0.5746333656948053,1.211733629537858,0.4940717453059738,0.6340356203723905,0.7879687060154597,0.8762353781581984,0.8477717565272004,0.2749317587153757,0.4405798476248409,0.8223902687751108,0.652565792831745,0.38262393094459063,1.0005895763773456,0.8093679503507738,1.050380837114825,0.5981407223223802,1.3653685693637696,1.053284718114268,0.6372182823674056,0.723586771227143,0.44120566522475024,0.33520812741200456,0.4278744523546698,0.3387076531089568,0.8981335820002079,0.7051134372854274,0.24302707946646274,0.18327813231352275,0.8624734592111882,1.1686270348168413,0.5568799510523975,0.7363449663922219,0.404568661637468,0.5726297285997524,0.2643464862189392,0.2805754723934753,0.356632155365582,0.4709254912470209,0.6240415101547103,0.591723842222674,0.6616545897567899,0.5778105448411303,0.38621090730981,0.5842538360702327,0.2068722481229402,0.5782753546281141,0.32779478782447946,0.6303991451653899,0.33237716330502853,0.23250602245923332,0.48440882097307225,0.5888857644257857,0.38677971966765795,0.6997499022670643,0.33363812094862766,0.15367928716731344,0.10975375339636116,0.24121905382940878,0.1911806552274647,0.5706076441477475,0.3675114667713321,0.5816504638243456,0.5172639046149154,0.30414566041647007,0.43790911192752996,0.8258905943942992,0.7886971586135185,0.908848249439122,0.43980240220142364,0.8250287478065147,0.570493803624625,0.12900201118083382,0.39401025645531634,0.37857423094391934,0.513199410540894,0.8347476636155617,0.6004155779568062,0.19766080802304764,0.3309199326706567,0.40113428858994726,0.5454838743414365,0.9092121502728623,0.4901403799701565,0.6591512954647728,1.4590462074843045,0.7945203346493531,0.6722293840331671,0.7831091260049905,0.732033957124738,0.4038539740682744,0.5070793770252522,1.0097920953138486,0.5804601372352853,0.9152222195445403,0.8390973626763188,0.7666331993591734,0.6087236092843036,0.35894167305396923,1.06992882994093,1.241365195564696,0.537582692002681,0.24283601161443338,0.7152626948388316,0.5795599950931216,0.7624930188559726,0.36172504041163256,0.366509823789264,0.43576475945955107,0.16447330878858127,0.36016844497334155,0.33748327447990517,0.4751131344875793,0.21191992993760617,0.479349567740211,0.7722590690676043,0.4465242228050197,0.23946193091257761,0.20426927268319744,0.44616103921318406,0.8629002958674692,0.8418797716081046,0.2659288556706223,0.3729349243302364,0.5426085380686338,0.36444644484926125,0.8763312346144505,0.4825119383544647,0.5463572221293186,0.632188260076944,0.4585091515248856,0.3093556488070176,0.2457476381539835,0.4176394811501872,0.5232274122975418,0.2700079069663144,0.4999332615851727,0.9121857374373189,0.351650153884644,0.9906105350839333,0.9109170526469997,0.6252017648835766,1.2856367209559212,0.7731432046828428,0.5782619690366261,0.7462108662615932,0.9814514315612373,0.7389881085314871,0.4224625074000902,0.29600034951491583,0.3331522659641039,0.5884611541335687,0.28153494649913713,1.0054443402076296,0.4958455064836707,0.3468479803243584,0.4996562068195044,0.41701983917766805,0.8031159323095622,0.5790104152246324,0.3842168063019067,0.7275020359869473,0.6308749141689405,0.28762133546282287,0.7879093737987368,0.8392022365130494,0.8050347848423527,0.8902241395952493,0.6435887745138243,0.7570808781166982,0.4199264644600602,1.0267588148863174,0.711984381524934,0.4093484328571332,0.3310786079560915,0.5358992215718379,0.28417946991759857,0.4295202568405274,0.8839520717993306,0.7580735618703028,0.4261464582525594,0.6781802317958556,0.8869574318128253,0.5355853517510055,0.6034982808416158,0.7241187031792067,0.7758858521215586,0.9962242323005783,0.7667521122682743,0.3967626967008494,0.45297454911703955,0.27515031439921744,0.47450705355308764,0.7187217743990437,0.9275963044151427,0.5867723109479679,0.3983833339456746,0.4807795363545834,0.3445892859685723,0.17079731526339625,0.7677363363175049,0.7246409091762067,0.6018564293427202,0.2818957766510634,0.5124397609790214,0.19940338877133998,0.037836074022119096,0.0560707543974345,0.27545355676059985,0.5241005643277724,0.568173925953131,0.580837800880133,0.3669176411648952,0.222657724097286,0.7209199269062034,0.9583713208617981,0.4670735261691227,0.9175596550448581,0.7221544173686871,1.1311368414890663,1.0969373076334326,0.4205522183310495,0.45076225998571406,0.6943729298844293,0.4641988793229523,0.255181524383133,0.42954976462115096,0.26572648948064337,0.3994892427636527,0.2707007003087527,0.7757170076318338,0.8268891810443346,0.8114981901908271,0.8103419076399678,0.6823124436162242,0.659563981797345,0.49210658797693463,0.5187364090945138,0.6588735347240454,0.5706354797182375,0.48621778697209594,0.5964050438796603,0.45572656558997565,0.1834034579153372,0.4065532347911459,0.17317335562666059,0.6513370789228996,1.3607164454807426,0.6652158288386381,0.49012850265036734,0.8008761460804498,0.38898596553483755,0.5789757404730742,1.0830246875265133,1.2291833217131232,0.44078278010180616,0.7302876664343134,0.882647987667571,0.3749764656462962,0.6785281094579486,0.5154253905763633,0.8519731943146334,0.5357816873463432,0.3586755843029722,0.4530154311582371,0.31415703641789333,0.3959671224506828,0.6805144397917952,0.7211159745272784,0.6387619238092324,0.8656399098678791,0.7170092621587805,0.3860548032185095,0.3015849746244378,0.7886953353789592,1.0108428950149384,0.30371455204101855,0.26105241914070026,0.257367828605642,0.29526081145632876,0.8963604344812359,0.4488310201688085,1.0059150212632166,1.0648028147723418,0.24185474600658363,0.3681327832924751,0.12529448254366415,0.07641765790472722,0.2367465388969558,0.5078760606540882,0.8652863159644857,0.420547244437117,0.8347286386207549,1.1643789830578997,0.6946726487349235,0.2027185048454016,0.39041575419768537,0.35901428985558853,0.6062406940731491,0.28905081173552993,0.729329478570539,0.7015499354079354,1.035582376852121,0.43067327639190434,0.7579685227811546,0.8294516296440281,0.19241473838680126,0.3156743867406361,0.3036236594302879,0.3240914722158152,0.48700384209497694,0.3805585278870191,0.9007367994085254,0.480233839532353,0.378711321472325,0.2421322075148839,0.21171331699721083,0.2131984294142621,0.7015426972733995,0.5374313888669375,0.3885754659225525,0.532419002203322,1.0945461330380315,0.918946858909461,0.43054462917231584,0.7618254894768886,1.0466869940775472,0.5525088755875605,0.2637591628923324,0.46568592691807575,0.732485610914913,0.3322658788766909,0.6334845441644932,0.6340350206363505,0.2906706284063311,0.9325394642340021,1.1075142376320404,0.2998341738752099,0.17352864681334634,0.3484608818518683,0.4366787505083577,0.6674159952480008,0.5912411606164285,1.0531698345028269,1.4266575435148416,0.8462414229478906,1.0595548571207134,0.9463893371849547,1.311667023472872,1.0987213924784385,0.793309506014032,0.8862335622679482,0.9208513704165242,0.9151285921854311,0.6479903118064103,0.9158959289345785,0.665078737069697,0.42387898171356164,0.3669175258834334,0.4975309981428552,0.5872931251395241,0.26348938957163504,0.4027318162723005,0.9303104422658866,0.953137861244832,0.6403422223857168,0.27778212614283,1.15819041142738,0.9730391645462548,0.729427181704072,0.4227925137048789,0.31156232647834814,1.0884392965073526,0.657099928969492,0.08626016373636133,0.4013642790766402,0.6903964896282196,0.5034581666495024,0.3413152216506457,0.2904782109120158,1.0720251370709464,1.3914530789987478,0.7916016578767215,0.39353027285084297,0.4722870403701439,0.7398076125763609,1.0104147402648482,0.8924171530306155,0.5621281987243937,0.581939761900166,0.336759795418564,0.59267428142601,0.6359950737210649,0.41658727336136137,0.5932602620019156,0.843250798569406,1.169024840955324,0.9184822331092125,0.42819474318046885,0.83143272446076,0.8290027992511015,0.5880337189749529,1.1329861808679766,0.40034214363586546,0.9406858864281369,0.7437617822248627,0.3155691803376796,0.4432008940696584,0.916200817902215,0.7624172492681729,1.1721264360728145,0.7050847374516056,0.704495093807648,1.170452426884655,0.4576106590452277,0.2754521625871358,0.4898121262756743,0.8927235058025922,0.837488378698207,1.4209747898592442,0.4495494928025957,0.5721011385043603,1.3145364272086701,1.2398958377203224,1.0712376376921446,1.2110319319581446,0.49156476349683,1.0078015417585096,0.9928687344565958,0.7632630441276409,0.9903633573042834,0.6560782252891744,1.083370427403123,0.8761218641248707,0.9692603013003959,0.953406928755197,0.4082932056074709,0.8364771570348005,1.1349577808330298,0.5593879888866229,0.5330534288658909,0.585185857349958,0.4254015892642143,1.066437995896388,1.0826687554563146,0.511935259545132,0.45002373955977215,0.5652646270902224,0.48511968460715904,0.7388693777424697,0.5173261944380274,0.6848522336268685,0.751687875202851,0.3405359537214131,0.5826064764028831,0.7987108464799407,0.8856434569536391,0.5284698318925163,0.5648084442191361,0.45558570193427145,0.41950562279988096,0.617893001044058,0.7354894578553639,0.5622237594342729,1.0734585983330658,1.0934905050930452,0.5565872277653401,0.8903019042134386,0.6209318627710787,0.22516199615003119,0.5376483339055596,0.28144218665852244,0.37350548651141324,0.6474602385567714,0.6992677734418337,0.29426627511669606,0.5256399103011311,0.37792455087866794,0.5250104951527248,0.7912879299394395,0.6009595948312473,1.365255400728211,0.9454098714487259,0.7669034996416457,0.5327894569974606,0.411868510728293,0.7731325543876882,0.35691825445179914,0.9086741669359291,0.6031082804842015,0.5606355896783707,0.23013498162299478,0.7722291871617786,1.1397103533463566,0.8883137060509039,0.7600454641180716,0.9976889680385013,1.1646691364521475,0.6962197790178606,0.5768841246959469,0.8468833951689194,1.113378184110922,0.26928041325547125,0.3880522794919941,0.3851513888147914,0.7039953384639759,0.7810942749410746,0.8113092066633021,0.42888349707015794,0.6542012138914226,1.0003428630022557,0.8773256771351601,0.43126227194986055,0.7724303319401282,0.6851725239618958,0.2397462413564819,0.6736126957529922,1.130139377716179,1.0553746791068046,0.6956923420642614,0.9430142718701913,1.0499395205997823,0.7838701292499963,0.43397362433770137,1.2858286055265349,0.9428785083823379,1.304040920886359,0.9294980679489999,1.187966204215801,0.4773910491643093,0.43178847529898967,0.6489323337118721,0.1916871309556905,1.1705361432272006,0.600384020568884,1.2248664916393104,0.8304220526735308,0.370267309222382,0.21717783292551254,0.22543803031278964,0.4487354460549244,0.3465339218038938,0.5070250553028306,0.5261544522486596,0.9438273015623533,0.6320808948353891,0.425342578110242,0.670534753914592,0.8517603274926616,0.8437346648763846,1.11254906536688,0.5031622491198092,0.9528413879846342,1.0230137161864208,0.22539073992415015,0.28406446689647125,0.4269303749534211,0.7735235167751031,1.5382926453896084,1.2514838220381757,0.638884860610079,1.130444200108038,0.3133560210072996,0.485033166571031,0.9493587078358745,1.0384532730884253,0.7570250692123944,0.9913116821424515,0.6026951137928931,0.4055109988832133,0.22258043195987404,0.4692181572796832,0.5582623912147402,0.467896273355322,0.33383209277189685,0.7332169803905217,0.35263166695371684,0.9484431481666735,0.658246231590017,0.07894976216739595,0.38590202075975866,0.6628933897545819,0.8178036743880917,0.6427380057679015,0.39389912530346427,0.7345924198921578,0.9424909696155761,0.8738450806335141,1.7817019115608024,0.8377852445210681,0.5304701110026397,0.5240554488598047,0.7881563417854821,0.6302117233338077,0.3966086653603175,0.778960039894014,0.5352301918899639,0.19153204874935406,0.21279784152814735,0.12751440770576866,0.14401804802437185,0.4552026936923488,0.5536402908025939,0.2422349522633743,0.6945615670429567,0.9609191393444719,0.624336183418205,0.6295834099635842,0.47132906947900166,0.3518391907045818,0.5751616464831297,0.6345202618613759,0.45877593688553914,0.6229960688916321,0.9718356190411135,0.6083529261624504,1.4923109969995811,0.6041860381622786,0.47808819307740963,0.3099150466446543,0.6517573660847622,0.6970336406826733,0.7945578141648065,0.38204959618389034,0.324519914615777,0.7717589725900359,0.39675100491073634,0.5404303611972747,0.5648558829028796,0.6421300326473449,0.24929878651173937,0.724517535628697,0.8958744379892669,0.28304122321251785,0.44309972564317734,0.43314403745163343,0.24820066884079928,1.0804042593782515,1.0625650525540575,0.37686433090022686,1.0719812609301802,0.8099017600895525,0.645461849099431,1.1008808329778246,0.4073396143408648,1.0971082859847918,1.022349478048314,0.4691198325318973,0.4576903996905228,0.33174141958753206,0.1504351030719168,0.3123892870571399,0.21266200601702007,0.5832723459825964,0.4376502351825178,0.4448023324490086,0.32606874764420707,0.6611783494675516,0.4563761668781069,1.003994444576741,0.43588767324755234,0.5255627843491836,0.6255627018219542,0.31472844935292443,0.37760749857074827,0.4943034517662241,0.7642857765844426,1.06193039336535,1.0479523426702868,0.7522218858990649,0.9061105635256284,0.6140325528390608,0.6593048945388471,0.6835801920274973,0.43500639298765875,0.22789092614936168,0.4630662049169044,0.8771257158197399,0.9430477292125744,0.3835421437316726,0.9619947486572147,0.805667943070841,0.2849843816218234,0.7681272227178515,1.1452491073115731,1.029988040167433,0.7149197942093634,0.5725741304624273,0.6727740927448607,0.4635179729709843,0.9267437371712263,0.9095994630070932,0.33758180260993687,0.7081378583075957,0.4813783986294546,0.779501445790143,0.5334043951687676,0.2998504728462198,0.293322468126302,0.6460225438546421,0.5732207328709555,1.0598326046562663,0.8848990796536154,0.6996342093457597,0.25700587455215,0.45875841286804153,0.18704621590972953,0.4717486951520549,0.7247714179255906,0.5891507973223178,0.3508624688510251,0.8760267361393881,0.6383185989296676,1.0508788861589051,1.0929324128642988,0.512902989227439,0.5886007279695958,0.9384133349314965,0.9575785152818718,0.4907790485748554,0.8774586384520205,0.20926472185190254,0.9992540651803445,0.5751343045066621,1.1115918434796144,0.41355969303817197,0.16214396775764545,0.8248324853701271,0.4561343095977349,0.7079417334228919,0.5996238102348407,0.7227644233964574,0.5540151870951953,0.41098564679695304,0.5201772790123304,0.5159289212588926,0.2916805903850163,0.522100814198696,0.8257535344624656,0.6966493078968956,0.33678564368718034,1.102969924674104,0.7312045281228292,0.48661014454101514,0.3199826060860993,0.3617873128694558,0.7919490896291779,0.31073238074957854,0.5597977436351456,0.37515238373362936,0.49998814725047225,0.6682249759471882,0.19284704779034947,0.3076279020357773,0.6767610478910697,0.6749788691080999,1.2527768255289602,1.2945245416193913,0.9151057156120741,0.8947152994504575,0.27833994214823377,0.30698447117095473,0.5029678235456855,0.8415389804042384,0.5482304517743783,0.9029512302836791,0.47146291371197524,0.19464837220732167,0.08175660455895012,0.21905748397367236,0.8624980020468882,0.8573975448997607,0.48558879043892494,1.1633449801057605,0.7012205668755288,0.15948174928980324,0.519970431626241,0.8753525312906909,0.481046093078096,0.9910150993067144,0.974605782687259,0.7355124310881618,0.3851701422784025,0.6862458045005579,0.8188389298926424,0.48757072592382955,0.6767199582628418,0.9417566241544804,0.6959499589253488,0.5209893625901938,0.5178310649103407,0.11858276632036713,0.8483774562395419,1.1180858862025964,0.41857669556476046,0.9856529781220167,0.9271703177260283,0.53614271406708,0.35586851699498867,0.3841996533524582,0.41837727646998274,0.8711159770059209,0.6839611458235165,0.7117825130616375,1.0022269437587028,0.4903666995294323,0.27141296999994974,0.2850920758279436,0.2574604909987798,0.5015605537525603,0.6257050390730201,0.6001124880059923,0.37065893844455144,0.40695985160645537,0.5667511140128791,0.47001892246876786,0.977031405040619,0.7630753501903645,0.5932652232133856,0.5338120177022352,0.2000135526201698,0.4159862568954986,0.5072109333729841,0.4947865885801578,1.1161217345989822,0.49651188561425763,0.8357200209152521,0.4947593048104374,0.6252370411843069,0.7676408927200549,0.6054511910326972,0.33348416664132285,0.37257618173635476,0.6190890009146043,0.6080499872758353,0.39281153082062614,0.7295910751241694,0.6270312434198971,0.3357135696548405,0.30188535827170904,0.17883237128032736,0.5307773128833619,0.46465597173497664,0.7270462473026205,0.8142175347122951,0.45612257994261457,1.0739981785660295,0.6709869326226781,0.5041812275299666,0.28001800981075575,0.49636296989071177,0.5084511482816331,0.4298405767153072,0.48654450071968397,0.5510984185291828,0.5405840509181048,0.3793137977635125,0.8878780900517458,0.8791027796458835,0.8126120909012624,0.5327169292523237,0.20176270858629053,0.26049845788186377,0.9147512802118584,0.8458014634838837,0.8473246610525298,0.5733619856050696,0.39769960224436846,0.8984334610686794,0.9867006745237636,0.8962693620373918,1.3336737954152187,0.7482772852338017,0.14848337987341453,0.3805644323504655,0.20458499992441959,0.6851342440222181,1.0852106313462617,0.7191784011078728,0.8649621519901631,0.49988016290642445,0.7300403621722424,0.5658887805506578,0.35350020943741267,0.8593602383451608,0.7830770787668587,0.12871342699322041,0.611297951502872,0.8531516929420071,0.31011802289915025,1.1002940253707159,1.0991235990471113,0.4939991162413512,0.43478133991409396,0.26726371041425995,0.3592565601121924,0.3707546367986503,0.21303832886485308,0.3317115676976468,0.6347326941443806,0.8019802694016427,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],\"yaxis\":\"y\",\"type\":\"scattergl\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"x\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"y\"}},\"legend\":{\"tracegroupgap\":0},\"margin\":{\"t\":60}},                        {\"responsive\": true}                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('60afbcec-c36c-4fa2-885d-59dceed1567a');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })                };                });            </script>        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "signal, target, bins = next(dataloader)\n",
    "pred = model(signal)\n",
    "print(f\"prediction: {torch.argmax(pred).item()}, target: {torch.argmax(target).item()}\")\n",
    "x = (bins[1:] + bins[:-1]) * 0.5  # get midpoints of bins\n",
    "signal = signal.cpu()\n",
    "y = signal.numpy()\n",
    "px.line(x=x, y=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c72977",
   "metadata": {},
   "source": [
    "### Creating confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3eb23159-ee96-4c70-9af3-42e543dc0886",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction: 0, target: 0\n"
     ]
    }
   ],
   "source": [
    "signal, target, bins = next(dataloader)\n",
    "preds = model(signal)\n",
    "print(f\"prediction: {torch.argmax(pred).item()}, target: {torch.argmax(target).item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a48b6976",
   "metadata": {},
   "outputs": [],
   "source": [
    "def confuse_me():\n",
    "    all_my_preds = torch.tensor([])\n",
    "    all_my_targets = torch.tensor([])\n",
    "    for _ in dataloader:\n",
    "        signal, target, bins = next(dataloader)\n",
    "        preds = model(signal)\n",
    "        all_my_preds   = torch.cat(all_my_preds, preds)\n",
    "        all_my_targets = torch.cat(all_my_targets, target)\n",
    "    print (preds)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "44c4f7dd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1492,    0],\n",
      "        [ 181,    0]])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Copied base for this from https://christianbernecker.medium.com/how-to-create-a-confusion-matrix-in-pytorch-38d06a7f04b7\n",
    "# Modified for use here\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sn\n",
    "\n",
    "\n",
    "from torchmetrics import ConfusionMatrix\n",
    "\n",
    "cf_preds = np.array([])\n",
    "cf_targs = np.array([])\n",
    "\n",
    "dataloader = example_loader(data_iter(\"./DATA/jsons/training_data.json.gz\"), in_dim)\n",
    "\n",
    "for _ in dataloader:\n",
    "    signal, target, bins = next(dataloader)\n",
    "    \n",
    "    output = model(signal)\n",
    "    \n",
    "    output = torch.argmax(output).data.cpu()\n",
    "    # output = output.type(torch.int64) # I don't think this is necessary, as it should already be int64\n",
    "    cf_preds = np.append(cf_preds, output)\n",
    "      \n",
    "    #target = target.data.cpu()\n",
    "    target = torch.argmax(target).data.cpu()\n",
    "    #target = target.type(torch.int64)\n",
    "    cf_targs = np.append(cf_targs, target) # Save Truth\n",
    "\n",
    "# Now convert from nparray to torch\n",
    "cf_preds = torch.as_tensor(cf_preds, dtype = torch.int64)\n",
    "cf_targs = torch.as_tensor(cf_targs, dtype = torch.int64)\n",
    "\n",
    "\n",
    "cf_matrix = ConfusionMatrix(task = \"binary\", num_classes =2)\n",
    "cf_matrix = cf_matrix(cf_preds, cf_targs)\n",
    "print(cf_matrix)\n",
    "\n",
    "sn.heatmap(cf_matrix/torch.sum(cf_matrix), annot=True, fmt=\".1%\", cmap=\"mako\")\n",
    "plt.savefig(\"./model/heatmap.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "93a39dda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1673)"
      ]
     },
     "execution_count": 293,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(cf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "a49da2d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[794,   0,   0,   0,   0],\n",
       "        [202,   0,   0,   0,   0],\n",
       "        [413,   0,   0,   0,   0],\n",
       "        [243,   0,   0,   0,   0],\n",
       "        [ 21,   0,   0,   0,   0]])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cf_matrix(cf_preds, cf_targs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd7ba1e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dafdb33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33552dca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2026b9ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc46da1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c07187b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a56d51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f376dc59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9970021",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513f3b62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "id": "89dc90d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3345\n"
     ]
    }
   ],
   "source": [
    "couuunt = 0\n",
    "for _ in dataloader:\n",
    "    couuunt +=1\n",
    "print(couuunt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
