# Code for making JSON files without the need for arraymaker.py
# Takes star IDs, obtains data, puts into format for JSON,
# and saves compressed JSON file

import json
import gzip
import numpy as np
import lightkurve as lk


def make_json(ids: list, target: int):
  """
  Uses lists of IDs generated from catalog modules
  to obtain DFTS, and prepares the data to be put into JSON files

  Parameters
  __________
  ids : list
      List of TIC IDs, generated by individual catalog modules.
      Must be added as only numbers, no "TIC" preamble.
      TIC ID expected as str.
  target : int
      Number associated with the classification, used for
      training and verification.
      If data is not for training or verification, use None.

  Yields
  ______
  List containing dictionaries with keys "TIC", "metadata", 
  "target" (the target # for training and validation), and
  frequency and power of each star.
  """
  
  # Remove IDs that come up as None using a filter
  print ("Removing Nones from list. Length of list before filtering:",
         len(ids))
  idsfiltered = filter(lambda item: item is not None, ids)
  ids = list(idsfiltered)
  print ("Nones removed, new length of TIC ID list:", len(ids))

  # Obtain DFTs fromm list of IDs
  stardata = get_dfts(ids)

  # Create list of dictionaries of all the data
  list_of_dicts = []
  for starid in ids:
      if starid == None:
        continue
      try:
        list_of_dicts.append({
                             "TIC":        starid,
                             "frequency":  stardata[str(starid + "_dft")
			                          ].frequency.value.tolist(),
                             "power":      stardata[str(starid + "_dft")
			                          ].power.value.tolist(),
    			     "metadata":   stardata[str(starid + "_lc")].meta,
                             "target":     target,
                          })
      except KeyError:
        continue
  # Remove quality mask from each, as it's unnecessary 
  # and wrong format for JSON
  for entry in list_of_dicts:
    try:
      if type(entry["metadata"]["QUALITY_MASK"]) == np.ndarray:
        del entry["metadata"]["QUALITY_MASK"]
    except KeyError:
      pass

  return list_of_dicts 


# Separate function for writing JSON, so training data 
# can be concatenated first if needed
def write_json(writedata: list, catname: str):
  """
  Writes data to JSON.

  Parameters
  __________
  writedata : list
    List of dictionaries
    Each dictionary contains star info, as well as dft data.
    Input for this should be created by make_JSON function.
  catname : str
    Name of the category of star/data.
    Only used for the name of the file written.
  """
  # catname can be "training" for training data
  with gzip.open(f"./jsons/{catname}.json.gz", "wt") as f:
    json.dump(writedata, f)
  print(f"json created for {catname} containing {len(writedata)} stars")



################ Putting training data into JSON ################
from KEBC.get_ebs import ebs
from get_dfts import get_dfts
from skarka.get_gdor_dsct import gdor, dsct, nvs
from rrlyr.get_rrlyrs import get_abs, get_rrcs
from dsct_murphy.get_extra_dsct import murphy_dsct

# Get all TIC IDs first
nv_ids    = nvs()
gdor_ids  = gdor()
dsct_ids  = dsct() + murphy_dsct()
eb_ids    = ebs()
rrlyr_ids = get_abs() + get_rrcs()

# Now get dfts and put into correct schema
nvs    = make_json(nv_ids, 0)
gdors  = make_json(gdor_ids, 1)
dscts  = make_json(dsct_ids, 2)
ebs    = make_json(eb_ids, 3)
rrlyrs = make_json(rrlyr_ids, 4)

# Put all training data together
all_training_data = np.concatenate((nvs, gdors, dscts, ebs, rrlyrs))
all_training_data = all_training_data.tolist()

# Save as training JSON file
write_json(all_training_data, "training_data")
